% !TEX TS-program = arara
% arara: xelatex: { synctex: on, options: [-halt-on-error] } 
% arara: biber
% % arara: texindy: { markup: xelatex }
% %% arara: makeglossaries
% % arara: xelatex: { synctex: on, options: [-interaction=batchmode, -halt-on-error] }
% % arara: xelatex: { synctex: on, options: [-interaction=batchmode, -halt-on-error]  }
% % arara: clean: { extensions: [ aux, log, out, run.xml, ptc, toc, mw, synctex.gz, ] }
% % arara: clean: { extensions: [ bbl, bcf, blg, ] }
% % arara: clean: { extensions: [ glg, glo, gls, ] }
% % arara: clean: { extensions: [ idx, ilg, ind, xdy, ] }
% % arara: clean: { extensions: [ plCode, plData, plMath, plExercise, plNote, plQuote, ] }
%-----------------------------------------------------------------
\documentclass[11pt]{PalisadesLakesBook}
% \geomHDTV
% \geomLandscape
\geomHalfDTV
\geomPortraitOneColumn
%-----------------------------------------------------------------

%\AsanaFonts % misssing \mathhyphen; less on page than Cormorant/Garamond
%\CormorantFonts % light, missing unicode greek
\EBGaramondFonts % fewest pages
%\ErewhonFonts
%\FiraFonts % tall lines, all sans, much less per page, missing \in?
%\GFSNeohellenicFonts 
%\KpFonts
%\LatinModernFonts
%\LegibleFonts
%\LibertinusFonts
%\NewComputerModernFonts
%\STIXTwoFonts
%\BonumFonts % most pages
%\PagellaFonts
%\ScholaFonts
%\TermesFonts
%\XITSFonts

%-----------------------------------------------------------------
\togglefalse{plMath}
\togglefalse{plCode}
\togglefalse{plData}
\togglefalse{plNote}
\togglefalse{plExercise}
\togglefalse{plQuote}
\togglefalse{printglossary}
\togglefalse{printindex}
%-----------------------------------------------------------------
\title{Notes on\\
 SICP,\\
  SICM,\\
   Functional Differential Geometry,\\ 
 Software Design for Flexibility, \\
 etc.}
\author{John Alan McDonald 
(palisades dot lakes at gmail dot com)}
\date{draft of \today}
%-----------------------------------------------------------------
\begin{document}
\maketitle
\PalisadesLakesTableOfContents{7}
%-----------------------------------------------------------------
\def\sharedFolder{../../shared/}
%-----------------------------------------------------------------
\begin{plSection}{Introduction}

Various works of Abelson, Sussman, et al.
 
Partially notes on reading; 
partially my own work-in-progress analysis 
and implementation in 
Clojure~\cite{
EmerickCarperGrand:2012:ClojureProgramming,
FogusHouser:2011:JoyOfClojure,
Halloway:2009:Clojure,
Hickey:2012:Clojure,
Rathore:2011:Clojure,
VanderhartSierra:2009:Clojure,}.

\citeAuthorYearTitle{HalfantSussman:1987:AbstractNumerical}

\citeAuthorYearTitle{AbelsonSussman:1996:SICP}

\citeAuthorYearTitle{SussmanWisdom:2013:FunctionalDifferentialGeometry}

\citeAuthorYearTitle{SussmanWisdom:2015:SICM2}

\citeAuthorYearTitle{HansonSussman:2021:SDFF}

%-----------------------------------------------------------------
\end{plSection}%{Introduction}
%-----------------------------------------------------------------
\begin{plSection}{\citeAuthorYearTitle{AbelsonSussman:1996:SICP}}

I have worked thru this book in Common Lisp and Clojure.
It changed my life, and it's my reason for trying the others here.

However, I'm not going to work thru it again, 
at least for the moment,
though it might be worth a pass later, to collect ideas 
for my own writing.

\end{plSection}%{\citeAuthorYearTitle{AbelsonSussman:1996:SICP}}
%-----------------------------------------------------------------
\begin{plSection}{\citeAuthorYearTitle{SussmanWisdom:2013:FunctionalDifferentialGeometry}}
\end{plSection}%{\citeAuthorYearTitle{SussmanWisdom:2013:FunctionalDifferentialGeometry}
%-----------------------------------------------------------------
\begin{plSection}{\citeAuthorYearTitle{SussmanWisdom:2015:SICM2}}
\end{plSection}%{\citeAuthorYearTitle{SussmanWisdom:2015:SICM2}
%-----------------------------------------------------------------
\begin{plSection}{\citeAuthorYearTitle{HansonSussman:2021:SDFF}}
This is the latest book chronologically,
but perhaps the content would place it between SICP
and the others?
%-----------------------------------------------------------------
\begin{plSection}{Short preview}

The strengths of SICP were substantive ideas, elegant code,
simple but real-enough applications.

This book on the other hand (at least up to 2.1.1)
has flaws similar to many software design books:
instead of real evidence in the form of convincing examples,
resort to logical fallacies (particularly appeal to
false authority/analogy)
and allusions to shallow fads.

I wonder if this is somehow due SICP falling from favor
and being replaced by Python at MIT.
And, if somehow therefore, they are descending to a level of
argument they think might appeal to mediocre students?

\end{plSection}%{Short preview}
%-----------------------------------------------------------------
\begin{plSection}{Forward (Guy Steele)}

Describes book as \emph{master class}, not tutorial.

Argues for bundling code plus data as \emph{closures}. 
Also \emph{generic functions}.

Not, perhaps, the clearest such argument, but how much can you do
in a 2 pages forward?

My argument would go something like:
Data is just bits, meaningless on its own.
The code that manipulates those bits determines their 
\emph{meaning}.
The farther the ``distance'' (in some sense) 
between code an data, 
the higher the likelihood of confusion,
of misinterpreting bits. 

\end{plSection}%{Forward (Guy Steele)}
%-----------------------------------------------------------------
\begin{plSection}{Preface}

One goal is to modify by addition, 
without changing existing code.

Begins talking about a very questionable analogy 
to biological systems.
Possibly a fallacy inherited from traditional AI.
Particularly ironic in the middle of a pandemic.

Something I believe:
\begin{plQuote}
{\citeAuthorYearTitle[p~xiii]{HansonSussman:2021:SDFF}}{}
We have often programmed ourselves into corners
and had to expend great effort to escape from those corners.
We have accumulated enough experience to feel that we can
identify, isolate, and demonstrate strategies
that we have found to be effective for building large systems
that can be adapted for purposes 
that were not anticipated in the original design. {\ldots}
\end{plQuote}
However, they may be over promising in some sense.

Sometimes, the best choice is starting over.
There are different levels of ``starting over'':
for example, complete re-design, 
re-implementing a mostly-the-same API, {\ldots}

My own experience is that, when having a choice
between starting from scratch and fixing the existing code,
the times I chose to start over were never a mistake,
and many, if not most, of the times I chose to ``fix'' existing
code turned out to cost more time than starting over would have.
I think I have always been more likely than most 
                to advocate starting clean,
and in my experience I've been too conservative in that direction.
The cause may be over-estimating the cost of something new
relative to the costs of maintaining/updating something that
already exists. THe reasons for that may have something to do
with the difficulty of associating costs to a design decision
that was made long ago.

Plus fear of the unknown: legacy code whose authors have vanished,
and no one present fully understands what it is doing or why.
The fear is that there is some lost lesson in the existing
code that will have to be painfully relearned in anything new.

We will see if the book has any to say about making such choices.

\end{plSection}%{Preface}
%-----------------------------------------------------------------
\begin{plSection}{Acknowledgements}
\begin{plQuote}
{\citeAuthorYearTitle[p~xiix]{HansonSussman:2021:SDFF}}{}
In many ways this book is an advanced sequel to SICP.
\end{plQuote}

Sussman acknowledges discussions (in graduate school?)
with Minsky and fellow students---possibly a source for the
biologically motivated old-style AI ideas that appear.
\end{plSection}%{Acknowledgements}
%-----------------------------------------------------------------
\begin{plSection}{1 Flexibility in nature and design}
%-----------------------------------------------------------------
\begin{plSection}{Summary of my reaction}

I find this chapter disappointing.

I think the fundamental flaw is using 
inappropriate analogies to other problem domains,
eg, (physical building) architecture
and, especially, evolution, 
to justify their ideas about software artifact design
and construction,
rather than arguing on first principles.
This is some version of the ``false analogy'' 
fallacy~\cite{wiki:ArgumentFromAnalogyFalse},
or perhaps the
``appeal to authority'' fallacy~\cite{wiki:ArgumentFromAuthority},
where the ``authority'' isn't talking about the problem at hand, 
and is not so clearly an ``authority'' in their own domain either.

These and similarly fallacious reasoning are common
in books about software design strategies, best practices,
philosophies, etc.

In software design, or any design problem,
it is difficult to usefully characterize what ``better'' means,
and hard to demonstrate to a skeptical observer that one
approach is ``better'' than another.
Hard enough to do with a reasonably objective observer,
and so much worse in areas like software design,
where people are heavily invested in particular technologies,
in time, money, and emotion.

I think this is a reason that so many fall into two rough classes
of fallacious reasoning. 

One is the appeal to false authority/analogy so prevalent in this 
chapter. 

In the perhaps more common version of this, the authors set 
themselves, or some other software ``guru'' as the authority, 
usually accompanied by oracular pronouncements left to the reader
to interpret.
This book is thankfully free from that.

The next most common is to appeal to some authority/analogy
from some other problem domain,
about which neither the reader nor the author know much.
The advantage of this, as (false) propaganda, 
is that the reader might doubt the pronouncements
of some ``guru'' that are falsified by their own experience,
but would be less confident questioning, for example,
dubious assertions about how evolution works.

The second class of fallacy is some variation on false 
quantification. 
In other words, pick some single criterion that is
easy to measure, easy to compare, and, ideally,
provides a total ordering. 
THe classic example is thinking more money is always better;
easy to tell that job A pays more than job B;
hard to tell which one really makes your life ``better''.

This chapter is essentially defending their ideas about software
design from fallacy type (2),
but they could do better.

They point out the error of premature optimization,
but miss the fact that start up costs are easier to measure
and, because they are closer in time, easier to attribute to
design choices. 
Long term maintenance costs are harder to measure and
associate with particular design choices.

The other problem with early optimization is that you don't
really understand what the code needs to do until you try to
use it in a real situation.

They are missing, what I think is one of the strongest arguments
in favor of their approach---the need for 
\emph{experimental/exploratory programming}.
\begin{itemize}
  \item Situations where you can fully specify what you need
  ahead of time---where you even understand what's 
  possible---are extremely rare.
  \item The faster and further you explore possibilities,
  the better the final result.
  \item If you don't do this, you are giving up one of the
  biggest advantages software has over other kinds of engineering:
  it is really easy to try different things.
  In this, I think there is a lot in common with math research.
  Hit a dead end? Erase the blackboard (delete files) 
  and start over.
  In physical engineering, say airplane design, you would have
  to scrap a $10^{6}$ dollar prototype and build another
  (and another and another {\ldots}).
  
  \item Sometimes people advocate 
  
  \item There is no final result. Context keeps changing.
  
\end{itemize}
Hanson and Sussman use the word \emph{evolvable}, 
to mean something similar, but not exactly the same,
and their use is confused by the false analogy to biology.
Evolution has no goal. 
Exploratory programming is about figuring out what the goal 
of the code can and should be.

Missing:
Software as written language; 
say just enough; avoid s/he problem.
Math (another written language) uses ambiguity
and implied context to reduce complexity,
to make (symbolic) calculations feasible,
within a few pages, blackboards, mortal mortal memory.
Is something similar possible or good in software? 

\end{plSection}%{Summary of my reaction}
%-----------------------------------------------------------------
\begin{plSection}{Initial text in ch 1}
They mention both screw fasteners and digital computers
as examples  of ``general purpose inventions'',
without saying what ``general purpose'' means.
There is obviously some limitation,
since a digital computer isn't very useful for holding two pieces 
of metal together, and a screw fastener won't help much in, say,
calculating your taxes. 
But the analogy really falls apart if you stop for a moment to
think about the fact that there probably on the order of 
$10^{5}\text{--}10^{6}$ different types of screw fasteners,
some  of which are highly optimized for specific applications,
and none of which cover all possible applications.
And the same is true, more or less, for digital computers,
though the number of distinct types is likely fewer.

I'm sure they know better, 
but their discussion suggests
specialized vs general purpose
and success vs failure are both binary.
Any tool will have some domain of tasks where it can be used
with varying degrees of success.
Possibly the argument here should really be about 
how (and when or if) making things more general increases
the probable degree of success. 

Possibly a better way to approach this
would be to talk about the need for humility in specifying
what a piece of software (or any tool) will be used for.
Enlarging the domain of applicability, in the right ways,
increases the chances that it will be useful when circumstances 
change, and, more important, 
when the first real use exposes all sorts of things 
you simply failed to think of ahead of time.
This is related in some ways to the ``Postel's law''
mentioned later, which I would re-word as:
functions should have large domains and small codomains.
But I think it's not really the same thing
(\TODO need a better word for problem ``domain"
that distinguishes it from function ``domain".
And examples would be nice.)

%-----------------------------------------------------------------
\begin{plSection}{Additive programming}
\label{sec:AdditiveProgramming}

Not crazy about the name.

Code can be flexible if it permits updates by \emph{replacing}
parts, not just \emph{adding} parts.
Common case: 
replace old implementation with (almost) equivalent 
higher performance one, and discard the slow version to ensure
it isn't invoked accidentally. 

In either case, proper abstraction makes it less likely
an update will break something that currently works.

What they advocate (p~2--3)
\begin{itemize}
  \item minimize assumptions about what a program will do.
  \item just-in-time decisions instead.
  \item whole more than sum of parts (this is almost automatic,
  depending on the meaning of ``sum''.)
  \item parts with separate concerns. (should really be groups of
  parts that share concerns, separate different groups.
  contradicts shared apis below.)
  \item minimize interactions (which contradicts to some degree
  wanting more than the sum of parts)
  \item parts ``simple and general'' 
  (but what does that mean? given two designs, how do you
  tell which is simpler and more general?)
  \item enlarge the (function) domain of ``acceptable'' inputs
  (which contradicts ``simple and general''?)
  \item reduce the codomain of possible outputs. 
  \item families of parts with same api (possibly on both sides
  of the api) (contradicts separate concerns, since sharing
  an api means sharing ``concerns'')
\end{itemize}

Postel's Law: Advocates robustness in the sense of
(1) accepting large domain of inputs
and (2) producing a small ``range of outputs of a part'' 
(codomain).
Not sure I believe this.
Depends on what ``accepting'' means in (1).
Often the right choice is to clearly reject unacceptable inputs
as soon as possible.
For example, if somebody asks for $\sqrt{\text{blue}}$
you generally don't want to quietly return \texttt{0x00000F}.

More concrete recommendations (p~3--5):
\begin{itemize}
  \item \emph{domain specific languages}, 
  which they equate to a family
  of ``mix-and-match parts''. I don't think that's 
  what most people mean by DSL, rather something more like
  syntactic sugar to make it easier to type common expressions
  in a problem domain. I've never been a fan of that notion of
  DSLs. Needing one is a symptom that you haven't gotten the
  abstractions (the mix-and-match parts) right. And it tends
  to create arbitrary syntactic barriers resulting in 
  ``impedance mismatch''.
  \item \emph{generic dispatch}. They require methods (``handlers'')
  to be associated with ``disjoint sets of arguments'', 
  which I don't think is always the right choice.
  They don't assume method choice is determined by class
  inheritance (good!).
  \item \emph{layer} data and procedures. Not at all clear what 
  that means (yet). Something about metadata.
  Example of numerical data with units.  
  \item combine multiple sources of (independent) 
  \emph{partial information}. 
  Also not clear what it means (yet).
  Example of combining parallax and brightness, etc., to estimate
  distance to star. 
  Not clear what's partial about this.
  Sounds like algorithms rather than
  program design techniques, and the idea that this can be done
  in an application-independent way, without careful
  testing and tuning in each case, seems naive.
  We'll see {\ldots}.
  \item \emph{degeneracy}. again not clear from brief description.
  They say that its' ``dual'' to partial
  information, but actually sounds like the same thing.
  Perhaps the idea is multiple procedures for computing the same
  thing, rather than multiple sources of data.
  (I really hate it when people use ``dual'' with specifying
  what they mean, in a way that seems like they just
  think it sounds cool, 
  and haven't thought thru what the point is.)
\end{itemize}

Last couple paragraphs on p~5:
Mentions upfront cost of flexible design vs
lifetime maintenance.
I think this could be strengthened.
A point that could be added is that 
it is easier to see the effect of design decisions 
on the cost/time of the initial implementation,
and harder to connect them to maintenance costs, 
often years later, with no one around who even remembers 
what those choices were. 

\end{plSection}%{Additive programming}
%-----------------------------------------------------------------
\end{plSection}%{Initial text in ch 1}
%-----------------------------------------------------------------
\begin{plSection}{1.1 Architecture of computation}

``A metaphor from architecture may be illuminating for the kind
of system that we contemplate.''

My appeal-to-false-authority alarms are going off.
The appeal to physical architecture, pattern languages, etc.,
is commonplace in writings on software design,
but I have always found it suspect.
The idea that a field 
where the constraints and degrees of freedom
are so fundamentally different should be imitated 
by software design is questionable at best.
As one who has had to deal with many heavily designed
and consequently dysfunctional buildings,
I find the idea that architects are actually good at what they do,
in any real sense, dubious.
See for example \citeAuthorYearTitle{Rybczynski:1986:Home},
which is a reference I just happen to have read some years ago.
(\TODO quote the forward?)

The remainder of the section is a confusing description 
of the term ``parti''.
And I don't think there is anything that wouldn't have been 
clearer if given directly in terms of software,
without the appeal to (physical) architecture as a model. 

\end{plSection}%{Sec~1.1 Architecture of computation}
%-----------------------------------------------------------------
\begin{plSection}{1.2 Smart parts for flexibility}

Starts with discussing the difficulty of specification
in the face of complexity. 

But, how do you measure complexity?
Example: how do you specify that a program
``plays a \emph{good} game of chess''?
But the issue there isn't complexity.
Possibly it is intrinsic ambiguity: disagreement about what
``good'' means.
I'm not sure that chess is the best example of that.
Most would agree winning is better than losing.
I suppose you could get some ambiguity out of lack of
total ordering: 
program A might (usually) beat program B,
which might (usually) beat program C, 
which might (usually) beat program A {\ldots}.

Then another appeal-to-false-authority,
in this case, with a claim the biological systems
are inherently robust and adaptable
(ironic in the middle of a pandemic).
Not clear at this point exactly what that implies for software
design.

``One idea is that biological systems use contextual signals
that are informative rather than imperative.
There is no master commander saying what each mart must do;
instead the parts choose their roles based on their surroundings.''

They seem to be advocating some communicating agents model
of computation.
The point is lost: at one point they are arguing for
``smart parts'', but also ``simpler'' and ``more general'',
all of which conflict. depending on the meaning of those
undefined terms. 
The analogy to biological systems falls apart with another
moment's thought: 
individual cells might be considered ``general'',
at least when they are stem cells,
but I don't think many would accept calling them ``simple''
or ``smart''.

This is followed by another false analogy to social systems:
democracy is better than autocracy, 
so software should avoid central controller designs.
I think there are good reasons for avoiding 
centralized synchronization,
and it doesn't need an appeal to democracy to justify it.
If anything the argument could go in the opposite direction:
it's easier to understand why synchronization and deadlock
are problems in software systems, 
which might help understand when similar designs 
might or might not be a good idea in social systems.
 
%-----------------------------------------------------------------
\begin{plSection}{Body plans}

``All vertebrates have essentially the same body plan.''

False authority alarm! 
They assume, without any evidence or argument,
that this ia a good thing, 
rather than just a contingent effect of 
evolution and constraints of biological development.
Rather than being an example of flexible design,
the shared body plan of vertebrates
is as or more likely evidence of natural selection
``painting itself into a corner''.

They then turn to design of radio receivers as another analogy.
The problem with this analogy is that all radio receivers
are doing pretty much the same thing,
and, even in this specialized case, 
it appears that there are a number of distinct ``body plans''.

They eventually get around to ``combinators''
and ``combinator languages''.
I've never been crazy about the terminology;
to me it makes something that's simple and obvious 
seem more difficult
and obscure than it needs to be.

The actual idea is good and important:
Systems are built out of things we think of as more-or-less
independent components;
the structures that combine components 
are themselves components,
(more-or-less) independent of each other 
and the things they combine,
and they can be further combined, ad infinitum {\ldots}.

I don't think you need any of the analogies to explain this
or justify it; I think they just get in the way.
And biological system don't have the same elegant recursive
structure, so if you took that as you model, 
you would miss something critical.

``Biological systems are universal in that each component can,
in principle, act as any other component. [p~11]''
This is just flat out false. 
Not even really true for stem cells, 
even at early stages of development,
and certainly not true later.

(And what's the implication for software?)

Section finishes by proposing an \emph{evaluator} as 
analog to stem cells:
``An evaluator takes a description of some computation
to be performed and input to that computation.
It produces the outputs that would arise if we passed the inputs
to a bespoke component that implemented the desired computation.
In computation we have a chance to pursue the powerfully
flexible strategy of embryonic development.
We will elaborate on the use of evaluator technology in chapter 5.''

This has so many problems, it's hard to know where to begin.
There are the usual, and unnecessary, appeal-to fallacies.
Perhaps more important, how is 
``a description of some computation to be performed"
different from 
``a bespoke component''? 
The stem cell analogy doesn't work, and obscures whatever
advantages and disadvantages an evaluator strategy might have.
THere are many stem cells,
and each re-configures itself
into something distinct and more specialized, 
in a one-way process.
The (almost always single) evaluator doesn't change, 
it just gets different computations to perform.
My guess is that the advantages are related to delaying
certain decisions about how to perform a particular computation,
following the general principle (not from biology or architecture)
that delaying a decision often gives you 
more information that can be used to optimize it.
Of course, delay isn't always better; 
sometimes waiting removes options.

\end{plSection}%{Body plans}
%-----------------------------------------------------------------
\end{plSection}%{1.2 Smart parts for flexibility}
%-----------------------------------------------------------------
\begin{plSection}{1.3 Redundancy and degeneracy}

Problems:
\begin{itemize}
  
  \item ``One of the characteristics of biological systems
is that they are redundant. 
Organs such as the liver and kidney are highly \emph{redundant}:
there is vastly more capacity than is necessary to do the job,
so a person missing a kidney or part of a [sic] liver suffers
no obvious incapacity.
Biological systems are also highly \emph{degenerate}:
there are usually many ways 
to satisfy a given requirement.''[p~12]
Usual unnecessary, false analogy.
  There are many obvious, better analogies, like airplane design.
  
  \item ``\emph{highly} redundant'',
  ``\emph{vastly} more capacity'' [emphasis mine]:
  Unless you could get by with, say,  less than $\frac{1}{10}$ 
  of a liver and kidney, I wouldn't say they are ``highly'' or
  ``vastly'' redundant.
  
  \item''\emph{degenerate}'': A few minutes looking at 
  dictionaries online finds no definition that matches this use.
  A guess is that this use is like ``dual'' earlier---a math-y
  sounding word that they haven't defined or thought thru
  carefully. What they mean is actually ``redundant'',
  and I guess they are trying to distinguish two kinds 
  of redundancy: excess capacity and and alternate mechanisms.
  
  \item They go on to describe the genetic code as ``degenerate'',
  which is yet another, distinct, non-standard, undefined meaning. 
  In this case, the issue is that
  the mapping from DNA to protein is not $\OneToOne$,
  multiple points in DNA space map to the same protein.
  They then confuse the many-to-one-ness of the mapping
  with a different, and not so obviously true, ``continuity" 
  property:
  small changes to DNA (often) result 
  in small changes in cell function.
  
  \item ``The theoretical structure of physics is deeply 
\emph{degenerate}.
For example, problems in classical mechanics can be approached
in multiple ways.''[p~12, emphasis mine]
This use is closer to their ``definition'',
but missing a key point. 
A better way to look at these things is to formulate 
the problem at the right level of abstraction,
and consider the choice between 
multiple representations/parameterizations of that abstraction
a computational detail.
(I think this may be why I never got very far 
in the first edition of SICM.)
For example:
\begin{plQuote}
{\citeAuthorYearTitle{MacLane:1954:Courses}}
{maclane:vspace}
Throughout these courses the infusion of a geometrical
point of view is of paramount importance. A vector
is geometrical; it is an element of a vector space, defined
by suitable axioms—whether the scalars be real numbers or
elements of a general field. A vector is not an n-tuple of
numbers until a coordinate system has been chosen. Any
teacher and any text book which starts with the idea that vectors
are n-tuples is committing a crime for which the proper
punishment is ridicule. The n-tuple idea is not ‘easier,’ it is
harder; it is not clearer, it is more misleading. By the same
token, linear transformations are basic and matrices are their
representations\ldots
\end{plQuote}

\item ``Engineered systems may incorporate some redundancy 
{\ldots}.
But they almost never incorporate degeneracy {\ldots},
except as a side effect of designs that are not optimal.''
Doesn't take much effort to show this is false.
Airplanes and nuclear power plants, among many other cases,
as a matter of course, incorporate many ``degenerate'' components,
where ``degenerate'' means 
``redundant distinct mechanisms for each critical task''.

\end{itemize}

At the end of this section, they refer to 
``combining partial information'' which will be covered in ch 7.
This confused discussion of ``redundancy'' and ``degeneracy''
is at best distantly related, and doesn't touch 
on the key issue: How do you combine?
Maybe ch 7 will be better {\ldots}.

\end{plSection}%{1.3 Redundancy and degeneracy}
%-----------------------------------------------------------------
\begin{plSection}{Exploratory behavior}

This section is similar to the preceding ones.
Inappropriate biological analogies that misunderstand the biology
and obscure the possibly valuable point.

The real idea is to compute something by searching a domain.
They seem to assume search must be random,
but of course that need not be so.

Figure 1.2 illustrates two possibilities:
(1) generate and test, looping until the test returns success,
and (2) generate and filter, without feedback.
I suspect that there is a better, more general picture,
related to walking around a domain,
using data accumulated on previous samples to determine
the next step.

They finally mention ``backtracking'', to be discussed 
further in ch 4 and 5.

\end{plSection}%{Exploratory behavior}
%-----------------------------------------------------------------
\begin{plSection}{1.5 The cost of flexibility}

Better than the previous sections,
because they mostly drop 
the appeals to false authority/analogy fallacies.

Starts by saying general and evolvable systems,
 that use the techniques described (very poorly!) 
 in the previous systems, are expensive.
Not clear to me that that is true, at least not always,
and nothing concrete to justify it.

``Part of the problem is that we are thinking abut cost
in the wrong terms.''[p~17]
Good as far as it goes, but I think they are missing something
important.

A fundamental problem in software design, or any design problem,
is that it is hard to usefully characterize what ``better' means,
and hard to demonstrate to a skeptical observer that one
approach is ``better'' than another.
Hard enough to do with a reasonably objective observer,
and so much worse in areas like software design,
where people are heavily invested in particular technologies,
in time, money, and emotion.

I think this is a reason that so many fall into two rough classes
of fallacious reasoning. 

One is the appeal to false authority/analogy so prevalent in this 
chapter. 
In the perhaps more common version of this, the authors set 
themselves as the authority, 
usually accompanied by oracular pronouncements left to the reader
to interpret.
This book is thankfully free from that.
The next most common is to appeal to some authority/analogy
from some other problem domain, about which 
neither the reader nor the author know much.

The second class of fallacy is some variation on false 
quantification. 
In other words, pick some criterion that is
easy to measure, easy to compare, and, ideally,
provides a total ordering. 
THe classic example is thinking more money is always better;
easy to tell that job A pays more than job B;
hard to tell which one really makes your life ``better''.

This chapter is essentially defending their ideas about software
design from fallacy type (2),
but they could do better.

They point out the error of premature optimization,
but miss the fact that start up costs are easier to measure
and, because they are closer in time, easier to attribute to
design choices. 
Long term maintenance costs are harder to measure and
associate with particular design choices.

The other problem with early optimization is that you don't
really understand what the code needs to do until you try to
use it in a real situation.

There is a subsection titled ``The problem with correctness''.
I agree with the conclusion, but I don't think their arguments
are very good.

``We assert this discipline [requiring proofs of correctness]
makes systems more brittle''.
But they don't offer much to support this assertion.

The fundamental problem with achieving ``correctness''
by proving a program meets a specification is:
How do you know the specification is correct?
In some sense, you are really just re-writing the program
in a different language.
At most, you are removing some distracting details,
and specifying only results, rather than how to get there.
It is, in any case, restricted to tasks  where a person can
read and verify the specification by eye.

\end{plSection}%{1.5 The cost of flexibility}
%-----------------------------------------------------------------
\end{plSection}%{Ch 1: Flexibility in nature and design}
%-----------------------------------------------------------------
\begin{plSection}{2 Domain Specific Languages}

Some misleading/confusing  (in my opinion)
vocabulary, chosen, perhaps, beacuse 
each of these things has been something of a fad,
in certain subsets of the software world:
\begin{description}
\item[DSL]
As I mentioned in \cref{sec:AdditiveProgramming},
their description of ``DSL'' doesn't match 
what the term suggests to me
(and, I think, to most software developers):
restrictive syntactic sugar that reduces 
the unnecessary baroque boilerplate code, 
that results from choosing a bad base language.

\item[Nouns and verbs]
In \cref{sec:AdditiveProgramming},
they describe a DSL as a family
of ``mix-and-match parts''.
Here, they call it  
``an abstraction in which the nouns and verbs of the language
are directly related to the problem domain.''
They don't say what they mean by ``nouns'' and ``verbs'',
or give any reference.

A quick search shows that this is a popular topic for web rants,
But I haven't found anything very insightful.
It looks like yet another misleading metaphor/simile
that lets people think they've gotten some easy insight,
when, in fact, it's just wrong.
Functions aren't verbs and objects aren't nouns,
even superficially.
And many natural languages don't  have clearly distinguished 
nouns and verbs (eg Chinese and English!).

I'm not sure where the ``noun/verb $=$ object/function'' 
metaphor originated,
maybe \citeAuthorYearTitle{Yegge:2006:Nouns}?

\item[Combinator]
This is burying the lead. 
Doesn't mean much.
Essentially just a function that takes functions as arguments,
and returns a function as value.

Real issue is functions as ``first class'' values in the 
language~\cite{wiki:FirstClassValue}.
To what extent are functions first class in Scheme?
Not fully first class in Clojure because the 
{\clojureFont clojure.lang.IFn} interface only covers applying 
a function to arguments.
You can't for example, easily find the name of a function,
determine if a function was created 
by composing other functions,
what those other functions were,
determine if a function is a closure,
what the lexical environment of the closure is,
etc.

Key properties needed in the language for ``combinators'':
\begin{itemize}
\item Functions are values that can be passed to other functions
as arguments.
\item Functions are values that can be stored (referenced)
in data structures.
\item New functions can be created at runtime, that reference
a new instance of some data structure,
usually containing functions.
\end{itemize}
\end{description}

Deserves more discussion first-class-ness and artificial barriers
between what any programmer can do and what only
the language implementer can do.

Security argument false. 
Important to be able to impose constraints on
who can read/create/execute/modify what. 
Doing this in the language definition
is neither necessary nor sufficient.
%-----------------------------------------------------------------
\begin{plSection}{2.1 Combinators}

Iterated line search optimization as a collection of better, less shallow, 
examples.

%-----------------------------------------------------------------
\begin{plSection}{2.1.1 Function combinators}

Preview: 
\begin{itemize}
  \item Use of ``combinator'' seems like a shallow allusion
  to fashionable terminology, no depth.
  \item Fail to give any convincing example of a ``combinator''
  in anything like a real application.
  \item The real idea is ``functions that manipulate functions'';
  they are missing any consideration of what that implies, 
  especially in terms of functions as first class objects.
  \item What's particularly missing is any thought about
  what a ``function'' is, eg, domain, codomain, etc.
  \item All the input/output arity stuff doesn't 
  work, certainly not in Clojure, and I'm pretty sure, 
  not in Scheme, because functions don't have a specific input
  arity in either, Clojure doesn't have multiple value return,
  and I don't think Scheme necessarily has a fixed number of
  multiple values returned. (And the Scheme examples rely on
  features of MIT Scheme which aren't present in other dialects!)
  \item They don't even point out that multiple arity input and
  multiple value output are the same thing!
  \item A better approach would be to stop and first talk
  about what a function is: takes one element of the domain
  and returns one element of the codomain
  (if it terminates, and maybe errors are distinct from the
  codomain?).
  \item Instead of talking about multiple arguments, 
  should talk about cartesian product domains and codomains.
  This would lead nicely to a discussion 
  about value ambiguity in math, and the problems that arise
  when translating that into code. In this case,
  the problem is that math doesn't usually distinguish
  $\left(\Set{A} \times \Set{B} \right)\times \Set{C}$
  from
  $\Set{A} \times \left(\Set{B} \times \Set{C} \right)$
  or from
  $\Set{A} \times \Set{B} \times \Set{C}$,
  but {\clojureFont [[a b] c]}, {\clojureFont [a [b c]]},
  and {\clojureFont [a b c]} (to use a Clojure example)
  are different, need to be handled differently,
  and, almost surely, would be intended to mean something 
  different.
  I think many unconsciously assume cartesian product 
  is associative, but that probably doesn't carry over
  to tuples in software.
  \item The same issue comes up with {\schemeFont compose},
  and ought to be addressed.
  Would it make sense to be able to say 
  {\schemeFont (compose f0 f1 f2)}?
  Do we want to distinguish that from 
  {\schemeFont (compose f0 (compose f1 f2))}
  and {\schemeFont (compose (compose f0 f1) f2)}?
  In Clojure, {\clojureFont (comp f0 f1 \& fs)}
  is \emph{variadic} (takes as many args as you want).
  
  \item Varying input arities, and output multiple values
  are just different \emph{representations} for elements
  of the (co)domain, some syntactic sugar for common cases.
  In other words, multi-argument input and multiple value output
  should both be understood as packing and unpacking lists
  (or some other data structure) which represent elements of the
  (co)domains. 
  \item The sugar should be considered (a) typing convenience
  and (b) a hint to the compiler that the argument/returned value
  lists will be very short lived, with very limited visibility,
  and should be implemented appropriately for this restricted use.
  \item Emphasizing the need for multiple different 
  \emph{representations} of elements of the (co)domain
  would lead nicely to generic functions. 
  \item {\schemeFont compose}  and {\schemeFont parallel-combine}
  both make sense. The remaining examples are confusing,
  bug prone, and rely on obscure language features.
  They could all be done better and simpler more robustly
  as compositions with (co)domain transformations.
  \item If the section focused on {\schemeFont compose},
  that could nicely lead to something like {\schemeFont decompose},
  aka factoring of functions, something which I have found
  extremely important in many contexts, and, I think,
  generally not appreciated.
\end{itemize}

\begin{plListing}
{anonymous {\schemeFont compose} in Scheme}
{composeSchemeA}
\begin{lstlisting}[language=scheme]
(define (compose f g)
  (lambda args
    (f (apply g args))))
\end{lstlisting}
\end{plListing}

\begin{plListing}
{named {\schemeFont compose} in Scheme}
{composeSchemeB}
\begin{lstlisting}[language=scheme]
(define (compose f g)
  (define (the-composition . args)
    (f (apply g args)))
  the-composition)
\end{lstlisting}
\end{plListing}

Is {\schemeFont the-composition} in the global environment?
Given a reference to the function object, can you find its name?
The name is pointless, even for debugging,
since it doesn't tell you what was composed,
so there will be many functions named 
{\schemeFont the-composition}.
Can you determine, from an arbitrary function object,
whether it is a composition?
Can you extract the factors of a closure?
Can you get at the lexical environment of a closure?

From this point, I'm only going to include Clojure versions
of the examples.
They won't be direct translations, but rather whatever I think
makes the most sense for the topic at hand.

However, in this case, a direct translation:

\begin{plListing}
{{\clojureFont compose} in Clojure}
{compose:Clojure}
\begin{lstlisting}[language=clojure]
(defn compose [f g]
  (fn the-composition [& args]
    (f (apply g args))))
\end{lstlisting}
\end{plListing}

Note: Like Scheme, Clojure functions are not fully first class.
The {\javaFont clojure.lang.IFn} interface is barely advertised,
and basically only supports applying a function to arguments.
There is no straightforward way 
to get at a named function's name.
Best option is parsing {\clojureFont (str f)},
but there's no guarantee the output format will stay the same.
Another possibility is finding the name in the function's 
metadata, but it's also pretty murky
what metadata goes with the {\clojureFont var},
and which with the function object
(and there may even be different metadata on the 
{\clojureFont var}'s symbol).

We don't need the translation of {\schemeFont compose}, 
because the equivalent {\clojureFont clojure.core/comp}
is built in.

However, because I want to be able to do more at runtime,
I will be implementing a parallel world of more first class
functions, including {\clojureFont compose}
as a generic function (see \TODO refs to relevant sections).
Making {\clojureFont compose} generic allows,
for example, optimizations in the case where we know enough
about the internals of the factors to reduce unnecessary
intermediate pushing and popping in the argument/value stack,
shared preliminary code, etc.
See \cref{sec:Compose} for more details.

\TODO should I adopt a naming convention to distinguish
generic from non-generic functions. Or should it just be assumed
that \emph{all} functions in my code are generic?
Can this be hidden in the implementation---normal functions
overwritten with a generic one on the first occurrence
of {\clojureFont defmethod}?

Straightforward translation from Scheme:
\begin{plListing}
{translated {\clojureFont iterate} in Clojure}
{iterate:Clojure}
\begin{lstlisting}[language=clojure]
(defn iterate [^long n f]
  (if (zero? n)
    identity
    (compose f (iterate (dec n) f))))
    
(test/is (= 390625 ((iterate 3 square) 5)))
\end{lstlisting}
\end{plListing}
\TODO say something about {\clojureFont test} library.

Clojure has {\clojureFont clojure.core/iterate} 
which returns an infinite lazy sequence:
{\clojureFont [(f x) (f (f x)) \ldots]}.
Using that we get:
\begin{plListing}
{native {\clojureFont iterate} in Clojure}
{native:iterate:Clojure}
\begin{lstlisting}[language=clojure]
(defn native-iterate [^long n f]
  (fn iterated [x] 
    (last (take (inc n) (clojure.core/iterate f x)))))
    
(test/is (= 390625 ((native-iterate 3 square) 5)))
\end{lstlisting}
\end{plListing}
\NOTE the potential for off-by-one errors between the Scheme
translation and the use of {\clojureFont native-iterate}.

%-----------------------------------------------------------------
\begin{plSection}{Arity}

I'm going to skip this section.

It is mixing two concerns that would be better separated:
\begin{itemize}
  \item Patterns (``combinators")
  for combining functions into new functions.
  Something like Clojure transducers might be a source of 
  better examples.
  \item Multiple arguments and return values. 
  They are often convenient, but they greatly complicate 
  the idea of what a function is, and, especially,
  combining functions.
  
\end{itemize}
Their first examples depend on assuming that each Scheme function
has a specific fixed number of both inputs and outputs, 
which is quickly exposed as false. 
In addition, it appears that there is no standard way 
in Scheme,
to get at the input and output arities, even assuming they are 
fixed.

Another issue in this section is the lack of insight
into the difference between {\schemeFont parallel-combine}
and {\schemeFont spread-combine}. 

\end{plSection}%{Arity}
%-----------------------------------------------------------------
\begin{plSection}{Multiple values}\label{Multiple:values}

I'm going to skip this section.

Clojure doesn't have multiple value return.
Idiomatic clojure handles this, and, to some extent,
multiple input arities as well,
by packaging data into nested sequences and hashmaps,
which, as a convenience, can be parsed with destructuring
argument specifications.
See \citeAuthorTitle{Clojure:Destructuring}.

The section also fails to provide much insight 
into the meaning g or lack thereof 
of allowing a function to return multiple values.

\NOTE
For symmetric functions (permuting the arguments doesn't
change the value), multiple inputs are relatively safe
and robust. 
For non-symmetric functions, 
arguments whose intended meaning is only indicated
by their position in a list
are brittle and dangerous.
As code evolves, it's common for the interface to a function
to need to support additional arguments,
and some of the old arguments become unnecessary.
If the only way to extend a function's arglist is by adding
new args to the end, you end up with something non-intuitive
and error prone.
That's directly relevant to the topic of the book,
and an example of the kind of insight that is missing.

In Clojure, this situation is commonly handle by packaging args 
into a hashmap. Not perfect, but much better 
than relying on position in the arglist alone.

\end{plSection}%{Multiple values}
%-----------------------------------------------------------------
\begin{plSection}{A small library}

I'm going to skip this section.

As they point out in exercise 2.4, 
the examples here are just various domain transformations composed
with the content functions,
obscured by multiple inputs, multiple outputs.

(\NOTE similarity 
``empty words'' 虚词 versus ``content words" 实词
in traditional standard Chinese grammar.)

\end{plSection}%{A small library}
%-----------------------------------------------------------------
\end{plSection}%{2.1.1 Function combinators}
%-----------------------------------------------------------------
\begin{plSection}{2.1.2 Combinators and body plans}

Appeal to false analogy\cite{wiki:ArgumentFromAnalogy}.

On second thought, maybe there is some accidental insight here.
The commonality of vertebrate body plans is almost surely
a consequence of evolution painting itself into a corner.

Using multiple-input multiple-output,
without any way to indicate meaning,
as the standard interface between functions
leads to brittle, painted-into-a-corner systems,
as noted in \cref{Multiple:values}.

\end{plSection}%{2.1.2 Combinators and body plans}
%-----------------------------------------------------------------
\end{plSection}%{2.1 Combinators}
%-----------------------------------------------------------------
\begin{plSection}{2.2 Regular expressions}

I'm going to skip re-doing this in Clojure, at least for now.
It seems like a bad choice of example---a mess with no useful
insights that I can see.

Standard (eg POSIX) regular expressions an example of a bad DSL.
Complaint seems to be that syntax is bad, rather than ideas.
But there very little discussion of what the ideas are.

Section proposes to improve syntax by writing a translator
of an (implied, but not discussed) better Scheme syntax
to a small subset of POSIX regexp strings.
Not clear what the point of that is supposed to be.
Would be more convincing 
if they actually implemented a pattern matcher.
Orr  at least had some application that demonstrated the 
advantages of their regexp syntax over POSIX.

Missing any discussion of what the purpose of pattern matching 
might be in a more general setting.
No big ideas.
No real analysis of what's wrong with POSIX syntax.
%-----------------------------------------------------------------
\begin{plSection}{2.2.1 A regular expression combinator language}

Lists a bunch of functions what the inputs and outputs are.
Describes what they do as ``matching'' 
without any explanation of what that means.

\end{plSection}%{2.2.1 A regular expression combinator language}
%-----------------------------------------------------------------
\begin{plSection}{2.2.2 Implementation of the translator}

Skipping for now. Messy. Not clear what the point is.
Bad choice of problem doesn't offer any useful insights 
into ``combinators'' or how to go about designing
a ``combinator language''.
Don't use it for anything in the end.

\begin{plSection}{The moral of the story}

``The moral of this story is that regular expression are 
a beautiful example of how \emph{not} to build build a system.
Using composable parts and combinators to make new parts
by combining others leads to simpler and more robust
implementations.''[p 43]

The problem with this statement is that the section doesn't
provide the substance to back it up.
To do that, they would need something like
(a) some analysis of what the real problem is, with non-trivial
motivating application,
(b) a sufficiently detailed design discussion covering choices
and their resolution,
and (c) a comparison with POSIX giving evidence of where it went
wrong.

\end{plSection}%{The moral of the story}
\end{plSection}%{2.2.2 Implementation of the translator}
%-----------------------------------------------------------------
\end{plSection}%{2.2 Regular expressions}
%-----------------------------------------------------------------
\begin{plSection}{2.3 Wrappers}

This application is much better than the previous one.
However, I think there are significant flaws 
in how they approach it.

Issues:
\begin{itemize}
  \item The data consists of bare numbers. It's up to the user
  to know what units of measurement a function expects,
  and what units it outputs, because there's no way to tell
  from the values themselves.
  They rely naming conventions to document what's intended
  (eg {\schemeFont celsius-to-fahrenheit}), asking for trouble.
  You can get by with that for simple conversions,
  but when you construct new ones via composition and inverse,
  watch out.
  \item  
\end{itemize}
%-----------------------------------------------------------------
\begin{plSection}{2.3.1 Specialization wrappers}
\end{plSection}%{2.3.1 Specialization wrappers}
%-----------------------------------------------------------------
\begin{plSection}{2.3.2 Implementing specializers}
\end{plSection}%{2.3.2 Implementing specializers}
%-----------------------------------------------------------------
\begin{plSection}{2.3.3 Adapters}
\end{plSection}%{2.3.3 Adapters}
%-----------------------------------------------------------------
\end{plSection}%{2.3 Wrappers}
%-----------------------------------------------------------------
\end{plSection}%{2 Domain Specific Languages}
%-----------------------------------------------------------------
%-----------------------------------------------------------------
\end{plSection}%{\citeAuthorYearTitle{HansonSussman:2021:SDFF}}
%-----------------------------------------------------------------
\begin{plSection}{Implementation}
I am stretching the work ``implementation'' to cover both
software and a mathematical formulation of the problems.
%-----------------------------------------------------------------
\begin{plSection}{Foundation sketch}

Math from idealized countable computation.

Idea: base mathematics on an ideal (Turing-like) machine.
Then mapping to usable software is more-or-less mapping
the ideal machine to a realizable one.

End result is something like constructive analysis
with no more than countable infinity, if that.
The key spaces in the readings will be built out of 
some version of computable reals.

See:

\citeAuthorYearTitle{Feferman:1989:IsCantorNecessary}

\citeAuthorYearTitle[ch.~2 ``Is Cantor necessary?'']{Feferman:1998:LightOfLogic}

\citeAuthorYearTitle[ch.~12 ``Is Cantor necessary? (Conclusion)'']{Feferman:1998:LightOfLogic}

\citeAuthorYearTitle{Feferman:1992:ALittleBit}

\citeAuthorYearTitle[ch~14 ``Why a little bit goes a long way:
logical foundations of scientifically applicable mathematics'']{Feferman:1998:LightOfLogic}

%-----------------------------------------------------------------
\begin{plSection}{An ideal machine}
\begin{itemize}
  \item Turing machine~\cite{
  Turing:1936:Computability,
  Turing:1937:ComputabilityLambda,
  Turing:1938:ComputabilityCorrection},
  Lambda calculus, etc.,
  equivalent,
  but more ``convenient''.
  \item unbounded (but not infinite)
  \item deterministic (?)
  \item references with tagged values
  \item write-once memory (but maybe that's not ``convenient'')?
  \item Primitive values plus data structures built out of tagged
  references.
\end{itemize}
\end{plSection}%{An ideal machine}
%-----------------------------------------------------------------
\begin{plSection}{An ideal language}
Ideal (formal) language. Lambda calculus but more ``convenient''.

Math based on procedures (procedural functions) 
in the ideal language.

Everything must handle the possible of non-terminating procedures
(on some or all inputs).
\end{plSection}%{An ideal language}
%-----------------------------------------------------------------
\begin{plSection}{(Procedural) Functions} 

A set is a procedure that returns true or false for any input
value.
As opposed to axiomatic set theory.
I think this eliminates the set paradoxes
that led to the ``crisis in mathematics'' circa 1900~\cite{
Feferman:2000:ConstructivePredicativeClassicalAnalysis,
Ferreiros:2008:Crisis}.

A proof is a procedure that takes a set of axiom expressions
and returns another (according to some rules).

%-----------------------------------------------------------------
\end{plSection}%{(Procedural) Functions} 
%-----------------------------------------------------------------
\begin{plSection}{Tuples} 

General tuple maps indexes to values.

Indexes most often in subsequence of $\Naturals$ starting from
$0$, $0 \ldots (n-1)$.

Indexes could be almost anything.

Tuple constructors look like
{\pseudocodeFont (tuple v0 v1 v2 {\ldots})} 
or
{\pseudocodeFont (kv-tuple k0 v0 k1 v1 k2 v2 {\ldots})}.

Tuple constructors are not associative, that is,
  {\pseudocodeFont (tuple v0 v1 v2)}
  $\neq$
  {\pseudocodeFont (tuple (tuple v0 v1) v2)}
$\neq$
  {\pseudocodeFont (tuple v0 (tuple v1 v2))}.


Likely to permit unbounded tuples, eg, ones where
{\pseudocodeFont (get tuple k)}
returns a value for any {\pseudocodeFont k} in the index set.

Problems:
\begin{itemize}
  \item What is {\pseudocodeFont (tuple x)}?
  In other words, what is a cartesian product 
  with only one factor and what are the elements of that set?
  Is it just {\pseudocodeFont x}? 
  Probably not. May be an error.
  But I think math books generally don't consider this edge case
  ($\times$ is assumed to a binary operator),
  and may  be implicitly identifying the single factor case
  with the original set.
  
  \TODO check the appropriate sections in,
  for example, \citeAuthorYearTitle{Halmos:1960:NaiveSetTheory}
  to see if he says anything explicitly of implicitly about
  the 1 factor edge case.
  
  Also, what is {\pseudocodeFont (tuple)}?
  Is there a general principle we can use to assign meaning
  to zero and single factor version of things that are normally 
  thought of as binary+ variadic operators?
  
  \item Curried constructors: 
  {\pseudocodeFont (tuple x0 x1)} 
  $\defeq$ 
  {\pseudocodeFont ((tuple x0) x1)},
  ie, {\pseudocodeFont (tuple x0)} is a constructor function
  that adds the next factor to the tuple.
  
  Note that we need to be careful about the difference between
  {\pseudocodeFont (((tuple x0) x1) x2) -> [x0 x1 x2]}
  and 
  {\pseudocodeFont (tuple (tuple (tuple x0) x1) x2) 
  -> [[[x0] x1] x2]}
  
  \item {\pseudocodeFont (get the-tuple k)}
   is a 2 argument function.
  We could try {\pseudocodeFont ((the-tuple k)}, treating the
  tuple as a function on its index set.
  
  However, this conflicts with currying the tuple constructor.
  Could be fixed with ``builder'' pattern:
  {\pseudocodeFont (finish-tuple ((((start-tuple) x0) x1) x2)) 
  -> [x0 x1 x2]}
  
\end{itemize}

\end{plSection}%{Tuples} 
%%-----------------------------------------------------------------
\begin{plSection}{(Procedural) Sets} 
-----------------------------------------------------------------
\begin{plSection}{Cartesian products} 

The elements of a cartesian product set are tuples
whose factors are elements of the corresponding set factors.
Out of all possible sets of tuples, cartesian product sets
are those that can be defined with a \emph{diagonal} predicate.
In other words, something like:
{\pseudocodeFont contains?} closure that looks like: 
\begin{plListing}
{{\pseudocodeFont contains?} closure}
{contains:clojure}
\begin{lstlisting}[language=pseudocode]
(defn contains?-method [set-tuple]
  (fn contains? [tuple]
    (reduce 
      and 
      (map 
        (fn [k] (contains? (set-tuple k) (tuple k)))
        (indexes set-tuple)))))
\end{lstlisting}
\end{plListing}
\TODO above not quite right until we deal with generic functions
and method definition/lookup. 

\NOTE {\pseudocodeFont contains?} is a 2-in function.
Maybe making everything 1-in is too much trouble?

Could be viewed as a tuple whose values are sets.

Note that a tuple doesn't doesn't \emph{belong} to any
particular cartesian product set; 
it is an element of any cartesian product set 
where the tuple factors are elements of the set factors.

\end{plSection}%{Cartesian products} 
%-----------------------------------------------------------------

\end{plSection}%{(Procedural) Sets} 
%-----------------------------------------------------------------
\begin{plSection}{Multiple arguments}

Main idea is importance of composing and factoring (de-composing)
functions, which is greatly complicated 
if functions take multiple arguments,
and, especially, if the number of arguments isn't fixed. 

Is there some reasonable way to separate these two concerns:
(a) (de)composing functions that map a domain to a codomain
and 
(b) functions that naturally operate on two or more inputs?

\TODO everything below needs to consider the case of zero
argument function calls as well. 
Is a zero argument function one whose domain is the empty set
$\emptyset$?
%-----------------------------------------------------------------
\begin{plSection}{Currying}

\begin{plListing}
{Currying}
{Currying}
\begin{lstlisting}[language=pseudocode]
(f a b c) $\defeq$ (((f a) b) c)
\end{lstlisting}
\end{plListing}

The problem with currying is that, with variadic functions,
you never know when you are done.

A common example is having
{\pseudocodeFont (add 1)} returns a function
that adds $1$ to its argument.
But then {\pseudocodeFont (add 1 2) $\defeq$  ((add 1) 2)}
must return a function that adds $3$ to its argument.
You never get a number out.

One answer to this is to identify the set of the possible
{\pseudocodeFont (add n)} with {\pseudocodeFont n},
which is elegant, but not practical for actual computation,
and, anyway, what then do you do about 
{\pseudocodeFont (multiply n)}, etc?

An alternative is to adopt something like the builder pattern:
each variadic function turns into two functions, 
a first that essentially just accumulates argument lists,
indefinitely, and a second that finishes the actual computation.
That is
\begin{plListing}
{Builder currying}
{Builder:currying}
\begin{lstlisting}[language=pseudocode]
(f a b c) $\defeq$ (finish-f ((((start-f) a) b) c))
\end{lstlisting}
\end{plListing}
Although it is possible {\pseudocodeFont start-f} 
is optimizing the accumulated arguments in some way,
it is most often going to just be accumulating the arguments,
so we could replace the above with
\begin{plListing}
{Accumulate currying}
{Accumulate:currying}
\begin{lstlisting}[language=pseudocode]
(f a b c) $\defeq$ (f ((((accumulate) a) b) c))
\end{lstlisting}
\end{plListing}
The builder pattern requires two functions instead of one,
but it does permit customization of argument accumulation.
As written above, we will need additional API for
{\pseudocodeFont accumulate} to extract the accumulated arguments.

\TODO
Consider how to deal with families 
of related, mutually dependent functions.

\NOTE
the similarity to {\pseudocodeFont (reduce sum map)},
clojure transducers.

\end{plSection}%{Currying} 
%-----------------------------------------------------------------
\begin{plSection}{Cartesian product sugar}

\begin{plListing}
{Cartesian product sugar}
{Cartesian:product:sugar}
\begin{lstlisting}[language=pseudocode]
(f a b c) $\defeq$ (f (tuple a b c))
\end{lstlisting}
\end{plListing}
which begs the question of how to interpret
{\pseudocodeFont (tuple a b c)}.
So maybe currying is the right answer.

\end{plSection}%{Cartesian product sugar} 
%-----------------------------------------------------------------
\end{plSection}%{Multiple arguments} 
%-----------------------------------------------------------------
\end{plSection}%{Foundation sketch}
%-----------------------------------------------------------------
\begin{plSection}{First class functions} 
%-----------------------------------------------------------------
\begin{plSection}{Composition}\label{sec:Compose}

\end{plSection}%{Composition} 
%-----------------------------------------------------------------
\end{plSection}%{First class functions} 
%-----------------------------------------------------------------
%-----------------------------------------------------------------
\end{plSection}%{Implementation}
%-----------------------------------------------------------------
\BeginAppendices
\input{typesetting}
%-----------------------------------------------------------------
%-----------------------------------------------------------------
\end{document}
%-----------------------------------------------------------------
