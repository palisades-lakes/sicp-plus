% !TEX TS-program = arara
% arara: xelatex: { synctex: on, options: [-halt-on-error] } 
% arara: biber
% % arara: texindy: { markup: xelatex }
% %% arara: makeglossaries
% % arara: xelatex: { synctex: on, options: [-interaction=batchmode, -halt-on-error] }
% % arara: xelatex: { synctex: on, options: [-interaction=batchmode, -halt-on-error]  }
% % arara: clean: { extensions: [ aux, log, out, run.xml, ptc, toc, mw, synctex.gz, ] }
% % arara: clean: { extensions: [ bbl, bcf, blg, ] }
% % arara: clean: { extensions: [ glg, glo, gls, ] }
% % arara: clean: { extensions: [ idx, ilg, ind, xdy, ] }
% % arara: clean: { extensions: [ plCode, plData, plMath, plExercise, plNote, plQuote, ] }
%-----------------------------------------------------------------
\documentclass[12pt]{PalisadesLakesBook}
% \geomHDTV
% \geomLandscape
\geomHalfDTV
\geomPortraitOneColumn
%-----------------------------------------------------------------

%\AsanaFonts % misssing \mathhyphen; less on page than Cormorant/Garamond
%\CormorantFonts % light, missing unicode greek
\EBGaramondFonts % fewest pages
%\ErewhonFonts
%\FiraFonts % tall lines, all sans, much less per page, missing \in?
%\GFSNeohellenicFonts 
%\KpFonts
%\LatinModernFonts
%\LegibleFonts
%\LibertinusFonts
%\NewComputerModernFonts
%\STIXTwoFonts\Spoa
%\BonumFonts % most pages
%\PagellaFonts
%\ScholaFonts
%\TermesFonts
%\XITSFonts

%-----------------------------------------------------------------
\togglefalse{plMath}
\togglefalse{plCode}
\togglefalse{plData}
\togglefalse{plNote}
\togglefalse{plExercise}
\togglefalse{plQuote}
\togglefalse{printglossary}
\togglefalse{printindex}
%-----------------------------------------------------------------
\title{Notes on\\
 Software Design for Flexibility}
\author{John Alan McDonald 
(palisades dot lakes at gmail dot com)}
\date{draft of \today}
%-----------------------------------------------------------------
\begin{document}
\maketitle
\PalisadesLakesTableOfContents{7}
%-----------------------------------------------------------------
\def\sharedFolder{../../shared/}
%-----------------------------------------------------------------
\begin{plSection}{Short preview}

\citeAuthorYearTitle{HansonSussman:2021:SDFF}

The strengths of SICP 
(\citeAuthorYearTitle{AbelsonSussman:1996:SICP})
were substantive ideas, elegant code,
simple but real-enough applications.

This book on the other hand (at least thru chapter 2)
has flaws similar to many software design books:
instead of real evidence in the form of convincing examples,
they resort to logical fallacies (particularly appeal to
false authority/analogy)
and allusions to shallow software fads/buzzwords.

I wonder if this is somehow due SICP falling from favor
and being replaced by Python at MIT.
And, if somehow therefore, they are descending to a level of
argument they think might appeal to mediocre students?

One thing in particular that seems to be lacking is 
any substantial discussion of \emph{design}.
They present (as of 2.3) a series of fait accompli examples.
What would be better is a more in depth discussion 
of the initial problem specification/preliminary ``requirements'',
including example of inconsistencies 
in the initial ``requirements'',
followed by some specific ideas about how those ``requirements''
might change,
followed abstraction/generalization that handles the specific
changes as well as others,
followed by a demonstration of what has to change in the code
when you do encounter a new ``use case'' to handle.

Using a questionable analogy myself:

If you want to avoid painting yourself into a corner,
you need to look at the room first, and come up with a plan
for painting the floor that ends at an exit.
And you also want to think about everything else 
you are might need to paint. 
You don't want to paint the floor,
and then be stuck waiting for it to dry before you can get
to the walls and the ceiling,
and possibly have extra work keeping the freshly painted floor
clean, which wouldn't have been necessary if you had painted
top down.

\end{plSection}%{Short preview}
%-----------------------------------------------------------------
\begin{plSection}{Front matter}
%-----------------------------------------------------------------
\begin{plSection}{Forward (Guy Steele)}

Describes book as \emph{master class}, not tutorial.

Argues for bundling code plus data as \emph{closures}. 
Also \emph{generic functions}.

Not, perhaps, the clearest such argument, but how much can you do
in a 2 pages forward?

My argument would go something like:
Data is just bits, meaningless on its own.
The code that manipulates those bits determines their 
\emph{meaning}.
The farther the ``distance'' (in some sense) 
between code an data, 
the higher the likelihood of confusion,
of misinterpreting bits. 

\end{plSection}%{Forward (Guy Steele)}
%-----------------------------------------------------------------
\begin{plSection}{Preface}

One goal is to modify by addition, 
without changing existing code.

Begins talking about a very questionable analogy 
to biological systems.
Possibly a fallacy inherited from traditional AI.
Particularly ironic in the middle of a pandemic.

Something I believe:
\begin{plQuote}
{\citeAuthorYearTitle[p~xiii]{HansonSussman:2021:SDFF}}{}
We have often programmed ourselves into corners
and had to expend great effort to escape from those corners.
We have accumulated enough experience to feel that we can
identify, isolate, and demonstrate strategies
that we have found to be effective for building large systems
that can be adapted for purposes 
that were not anticipated in the original design. {\ldots}
\end{plQuote}
However, they may be over promising in some sense.

Sometimes, the best choice is starting over.
There are different levels of ``starting over'':
for example, complete re-design, 
re-implementing a mostly-the-same API, {\ldots}

My own experience is that, when having a choice
between starting from scratch and fixing the existing code,
the times I chose to start over were never a mistake,
and many, if not most, of the times I chose to ``fix'' existing
code turned out to cost more time than starting over would have.
I think I have always been more likely than most 
                to advocate starting clean,
and in my experience I've been too conservative in that direction.
The cause may be over-estimating the cost of something new
relative to the costs of maintaining/updating something that
already exists. The reasons for that may have something to do
with the difficulty of associating costs to a design decision
that was made long ago.

Plus fear of the unknown: legacy code whose authors have vanished,
and no one present fully understands what it is doing or why.
The fear is that there is some lost lesson in the existing
code that will have to be painfully relearned in anything new.

We will see if the book has any to say about making such choices.

\end{plSection}%{Preface}
%-----------------------------------------------------------------
\begin{plSection}{Acknowledgements}
\begin{plQuote}
{\citeAuthorYearTitle[p~xiix]{HansonSussman:2021:SDFF}}{}
In many ways this book is an advanced sequel to SICP.
\end{plQuote}

Sussman acknowledges discussions (in graduate school?)
with Minsky and fellow students---possibly a source for the
biologically motivated old-style AI ideas that appear.
\end{plSection}%{Acknowledgements}
%-----------------------------------------------------------------
\end{plSection}%{Front matter}
%-----------------------------------------------------------------
\begin{plSection}{1 Flexibility in nature and design}
%-----------------------------------------------------------------
\begin{plSection}{Summary of my reaction}

I find this chapter disappointing.

I think the fundamental flaw is using 
inappropriate analogies to other problem domains,
eg, (physical building) architecture
and, especially, evolution, 
to justify their ideas about software artifact design
and construction,
rather than arguing on first principles.
This is some version of the ``false analogy'' 
fallacy~\cite{wiki:ArgumentFromAnalogyFalse},
or perhaps the
``appeal to authority'' fallacy~\cite{wiki:ArgumentFromAuthority},
where the ``authority'' isn't talking about the problem at hand, 
and is not so clearly an ``authority'' in their own domain either.

These and similarly fallacious reasoning are common
in books about software design strategies, best practices,
philosophies, etc.

In software design, or any design problem,
it is difficult to usefully characterize what ``better'' means,
and hard to demonstrate to a skeptical observer that one
approach is ``better'' than another.
Hard enough to do with a reasonably objective observer,
and so much worse in areas like software design,
where people are heavily invested in particular technologies,
in time, money, and emotion.

I think this is a reason that so many fall into two rough classes
of fallacious reasoning. 

One is the appeal to false authority/analogy so prevalent in this 
chapter. 

In the perhaps more common version of this, the authors set 
themselves, or some other software ``guru'' as the authority, 
usually accompanied by opaque oracular pronouncements 
left to the reader
to interpret.
This book is thankfully free from that.

The next most common is to appeal to some authority/analogy
from some other problem domain,
about which neither the reader nor the author know much.
The advantage of this, as (false) propaganda, 
is that the reader might doubt the pronouncements
of some ``guru'' that are falsified by their own experience,
but would be less confident questioning, for example,
dubious assertions about how evolution works.

The second class of fallacy is some variation on false 
quantification. 
In other words, pick some single criterion that is
easy to measure, easy to compare, and, ideally,
provides a total ordering. 
The classic example is thinking more money is always better;
easy to tell that job A pays more than job B;
hard to tell which one really makes your life ``better''.

This chapter is essentially defending their ideas about software
design from fallacy type (2),
but they could do better.

They point out the error of premature optimization,
but miss the fact that start up costs are easier to measure
and, because they are closer in time, easier to attribute to
design choices. 
Long term maintenance costs are harder to measure and
associate with particular design choices.

The other problem with early optimization is that you don't
really understand what the code needs to do until you try to
use it in a real situation.

They are missing, what I think is one of the strongest arguments
in favor of their approach---the need for 
\emph{experimental/exploratory programming}.
\begin{itemize}

  \item Situations where you can fully specify what you need
  ahead of time---where you even understand what's 
  possible---are extremely rare.

  \item The faster and further you explore possibilities,
  the better the final result.

  \item If you don't do this, you are giving up one of the
  biggest advantages software has over other kinds of engineering:
  it is really easy to try different things.
  In this, I think there is a lot in common with math research.
  Hit a dead end? Erase the blackboard (delete files) 
  and start over.
  In physical engineering, say airplane design, you would have
  to scrap a $10^{6}$ dollar prototype and build another
  (and another and another {\ldots}).
  
  \item There is no final result. Context keeps changing.
  
\end{itemize}
Hanson and Sussman use the word \emph{evolvable}, 
to mean something similar, but not exactly the same,
and their use is confused by the false analogy to biology.
Evolution has no goal. 
Exploratory programming is about figuring out what the goal 
of the code can and should be.

Missing:
Software as written language; 
say just enough; avoid s/he problem.
Math (another written language) uses ambiguity
and implied context to reduce complexity,
to make (symbolic) calculations feasible,
within a few pages, blackboards, mortal mortal memory.
Is something similar possible or good in software? 

\end{plSection}%{Summary of my reaction}
%-----------------------------------------------------------------
\begin{plSection}{Initial text in ch 1}
They mention both screw fasteners and digital computers
as examples  of ``general purpose inventions'',
without saying what ``general purpose'' means.
There is obviously some limitation,
since a digital computer isn't very useful for holding two pieces 
of metal together, and a screw fastener won't help much in, say,
calculating your taxes. 
But the analogy really falls apart if you stop for a moment to
think about the fact that there probably on the order of 
$10^{5}\text{--}10^{6}$ different types of screw fasteners,
some  of which are highly optimized for specific applications,
and none of which cover all possible applications.
And the same is true, more or less, for digital computers,
though the number of distinct types is likely fewer.

I'm sure they know better, 
but their discussion suggests
specialized vs general purpose
and success vs failure are both binary.
Any tool will have some domain of tasks where it can be used
with varying degrees of success.
Possibly the argument here should really be about 
how (and when or if) making things more general increases
the probable degree of success. 

Possibly a better way to approach this
would be to talk about the need for humility in specifying
what a piece of software (or any tool) will be used for.
Enlarging the domain of applicability, in the right ways,
increases the chances that it will be useful when circumstances 
change, and, more important, 
when the first real use exposes all sorts of things 
you simply failed to think of ahead of time.
This is related in some ways to the ``Postel's law''
mentioned later, which I would re-word as:
functions should have large domains and small codomains.
Also, not a good idea as stated:
promising a very small codomain is classic
``painting into a corner'',
causing endless torment when you discover the small codomain
isn't adequate
(eg promising {\pseudocodeFont price} is in US\$,
later adding a one-time conversion from euros to dollars,
leaving a system open to exploitation 
when the market rates change).
But I think it's not really the same thing
(\TODO need a better word for problem ``domain"
that distinguishes it from function ``domain".
And examples would be nice.)

%-----------------------------------------------------------------
\begin{plSection}{Additive programming}
\label{sec:AdditiveProgramming}

Not crazy about the name.

Code can be flexible if it permits updates by \emph{replacing}
parts, not just \emph{adding} parts.
Common case: 
replace old implementation with (almost) equivalent 
higher performance one, and discard the slow version to ensure
it isn't invoked accidentally. 

In either case, proper abstraction makes it less likely
an update will break something that currently works.
(\TODO give some argument for why this is true.)

What they advocate (p~2--3)
\begin{itemize}
  \item minimize assumptions about what a program will do.
  \item just-in-time decisions instead.
  \item whole more than sum of parts (this is almost automatic,
  depending on the meaning of ``sum''.)
  \item parts with separate concerns. (should really be groups of
  parts that share concerns, separate different groups.
  contradicts shared APIs below.)
  \item minimize interactions (which contradicts to some degree
  wanting more than the sum of parts)
  \item parts ``simple and general'' 
  (but what does that mean? given two designs, how do you
  tell which is simpler and more general?)
  \item enlarge the (function) domain of ``acceptable'' inputs
  (which contradicts ``simple and general''?)
  \item reduce the codomain of possible outputs. 
  \item families of parts with same api (possibly on both sides
  of the api) (contradicts separate concerns, since sharing
  an api means sharing ``concerns'')
\end{itemize}

Postel's Law: Advocates robustness in the sense of
(1) accepting large domain of inputs
and (2) producing a small ``range of outputs of a part'' 
(codomain).
Not sure I believe this.
Depends on what ``accepting'' means in (1).
Often the right choice is to clearly reject unacceptable inputs
as soon as possible.
For example, if somebody asks for $\sqrt{\text{blue}}$
you generally don't want to quietly return \texttt{0x00000F}.

More concrete recommendations (p~3--5):
\begin{itemize}
  \item \emph{domain specific languages}, 
  which they equate to a family
  of ``mix-and-match parts''. I don't think that's 
  what most people mean by DSL. 
  I think it is rather something more like
  syntactic sugar to make it easier to type common expressions
  in a problem domain. I've never been a fan of that notion of
  DSLs. Needing one is a symptom that you haven't gotten the
  abstractions (the mix-and-match parts) right. And it tends
  to create arbitrary syntactic barriers resulting in 
  ``impedance mismatch''.
  \item \emph{generic dispatch}. They require methods (``handlers'')
  to be associated with ``disjoint sets of arguments'', 
  which I don't think is always the right choice.
  They don't assume method choice is determined by class
  inheritance (good!).
  \item \emph{layer} data and procedures. Not at all clear what 
  that means (yet). Something about metadata.
  Example of numerical data with units.  
  \item combine multiple sources of (independent) 
  \emph{partial information}. 
  Also not clear what it means (yet).
  Example of combining parallax and brightness, etc., to estimate
  distance to star. 
  Not clear what's partial about this.
  Sounds like algorithms rather than
  program design techniques, and the idea that this can be done
  in an application-independent way, without careful
  testing and tuning in each case, seems naive.
  We'll see {\ldots}.
  \item \emph{degeneracy}. again not clear from brief description.
  They say that its' ``dual'' to partial
  information, but actually sounds like the same thing.
  Perhaps the idea is multiple procedures for computing the same
  thing, rather than multiple sources of data.
  (I really hate it when people use ``dual'' with specifying
  what they mean, in a way that seems like they just
  think it sounds like a cool mathy words, 
  and haven't thought thru what the point is.)
\end{itemize}

Last couple paragraphs on p~5:
Mentions upfront cost of flexible design vs
lifetime maintenance.
I think this could be strengthened.
A point that could be added is that 
it is easier to see the effect of design decisions 
on the cost/time of the initial implementation,
and harder to connect them to maintenance costs, 
often years later, with no one around who even remembers 
what those choices were. 

\end{plSection}%{Additive programming}
%-----------------------------------------------------------------
\end{plSection}%{Initial text in ch 1}
%-----------------------------------------------------------------
\begin{plSection}{1.1 Architecture of computation}

``A metaphor from architecture may be illuminating for the kind
of system that we contemplate.''

My appeal-to-false-authority alarms are going off.
The appeal to physical architecture, pattern languages, etc.,
is commonplace in writings on software design,
but I have always found it suspect.
The idea that a field 
where the constraints and degrees of freedom
are so fundamentally different should be imitated 
by software design is questionable at best.
As one who has had to deal with many heavily designed
and consequently dysfunctional buildings,
I find the idea that architects are actually good at what they do,
in any real sense, dubious, if not laughable.
See for example \citeAuthorYearTitle{Rybczynski:1986:Home},
which is a reference I just happen to have read some years ago.
(\TODO quote the forward?)

The remainder of the section is a confusing description 
of the term ``parti''.
I don't think there is anything that wouldn't have been 
clearer if given directly in terms of software,
without the appeal to (physical) architecture as a model. 

\end{plSection}%{Sec~1.1 Architecture of computation}
%-----------------------------------------------------------------
\begin{plSection}{1.2 Smart parts for flexibility}

Starts with discussing the difficulty of specification
in the face of complexity. 

But, how do you measure complexity?
Example: how do you specify that a program
``plays a \emph{good} game of chess''?
But the issue there isn't complexity.
Possibly it is intrinsic ambiguity: disagreement about what
``good'' means.
I'm not sure that chess is the best example of that.
Most would agree winning is better than losing.
I suppose you could get some ambiguity out of lack of
total ordering: 
program A might (usually) beat program B,
which might (usually) beat program C, 
which might (usually) beat program A {\ldots}.

Then another appeal-to-false-authority,
in this case, with a claim the biological systems
are inherently robust and adaptable
(ironic in the middle of a pandemic).
Not clear at this point exactly what that implies for software
design.

``One idea is that biological systems use contextual signals
that are informative rather than imperative.
There is no master commander saying what each mart must do;
instead the parts choose their roles based on their surroundings.''

They seem to be advocating some communicating agents model
of computation.
The point is lost: at one point they are arguing for
``smart parts'', but also ``simpler'' and ``more general'',
all of which conflict. depending on the meaning of those
undefined terms. 
The analogy to biological systems falls apart with another
moment's thought: 
individual cells are really only``general''
when they are stem cells,
and I don't think many would accept calling them ``simple''
or ``smart''.

This is followed by another false analogy to social systems:
democracy is better than autocracy, 
so software should avoid central controller designs.
I think there are good reasons for avoiding 
centralized synchronization,
and it doesn't need an appeal to democracy to justify it.
If anything the argument could go in the opposite direction:
it's easier to understand why synchronization and deadlock
are problems in software systems, 
which might help understand when similar designs 
might or might not be a good idea in social systems.
 
%-----------------------------------------------------------------
\begin{plSection}{Body plans}

``All vertebrates have essentially the same body plan.''

False authority alarm! 
They assume, without any evidence or argument,
that this ia a good thing, 
rather than just a contingent effect of 
evolution and constraints of biological development.
Rather than being an example of flexible design,
the shared body plan of vertebrates
is more likely evidence of natural selection
``painting itself into a corner''.
(Because of the way development works in vertebrates,
adding parts is much less likely than deleting them.)

They then turn to design of radio receivers as another analogy.
The problem with this analogy is that all radio receivers
are doing pretty much the same thing,
and, even in this specialized case, 
it appears that there are a number of distinct ``body plans''.

They eventually get around to ``combinators''
and ``combinator languages''.
I've never been crazy about the terminology;
to me it makes something that's simple and obvious 
seem more difficult
and obscure than it needs to be.

The actual idea is good and important:
Systems are built out of things we think of as more-or-less
independent components;
the structures that combine components 
are themselves components,
(more-or-less) independent of each other 
and the things they combine,
and they can be further combined, ad infinitum {\ldots}.

I don't think you need any of the analogies to explain this
or justify it; I think they just get in the way.
And biological system don't have the same elegant recursive
structure, so if you took that as your model, 
you would miss something critical.

``Biological systems are universal in that each component can,
in principle, act as any other component. [p~11]''
This is just flat out false. 
Not even really true for stem cells, 
even at early stages of development,
and certainly not true later.

(And what's the implication for software?)

Section finishes by proposing an \emph{evaluator} as 
analog to stem cells:
``An evaluator takes a description of some computation
to be performed and input to that computation.
It produces the outputs that would arise if we passed the inputs
to a bespoke component that implemented the desired computation.
In computation we have a chance to pursue the powerfully
flexible strategy of embryonic development.
We will elaborate on the use of evaluator technology in chapter 5.''

This has so many problems, it's hard to know where to begin.
There are the usual, and unnecessary, appeal-to fallacies.
Perhaps more important, how is 
``a description of some computation to be performed"
different from 
``a bespoke component''? 
The stem cell analogy doesn't work, and obscures whatever
advantages and disadvantages an evaluator strategy might have.
There are many stem cells,
and each re-configures itself
into something distinct and more specialized, 
in a one-way process.
The (almost always single) evaluator doesn't change, 
it just gets different computations to perform.
My guess is that the advantages are related to delaying
certain decisions about how to perform a particular computation,
following the general principle (not from biology or architecture)
that delaying a decision often gives you 
more information that can be used to optimize it.
Of course, delay isn't always better; 
sometimes waiting removes options.

\end{plSection}%{Body plans}
%-----------------------------------------------------------------
\end{plSection}%{1.2 Smart parts for flexibility}
%-----------------------------------------------------------------
\begin{plSection}{1.3 Redundancy and degeneracy}

Problems:
\begin{itemize}
  
  \item ``One of the characteristics of biological systems
is that they are redundant. 
Organs such as the liver and kidney are highly \emph{redundant}:
there is vastly more capacity than is necessary to do the job,
so a person missing a kidney or part of a [sic] liver suffers
no obvious incapacity.
Biological systems are also highly \emph{degenerate}:
there are usually many ways 
to satisfy a given requirement.''[p~12]
Usual unnecessary, false analogy.
  There are many obvious, better analogies, like airplane design.
  
  \item ``\emph{highly} redundant'',
  ``\emph{vastly} more capacity'' [emphasis mine]:
  Unless you could get by with, say,  less than $\frac{1}{10}$ 
  of a liver and kidney, I wouldn't say they are ``highly'' or
  ``vastly'' redundant.
  
  \item''\emph{degenerate}'': A few minutes looking at 
  dictionaries online finds no definition that matches this use.
  A guess is that this use is like ``dual'' earlier---a math-y
  sounding word that they haven't defined or thought thru
  carefully. What they mean is actually ``redundant'',
  and I guess they are trying to distinguish two kinds 
  of redundancy: excess capacity and and alternate mechanisms.
  
  \item They go on to describe the genetic code as ``degenerate'',
  which is yet another, distinct, non-standard, undefined meaning. 
  In this case, the issue is that
  the mapping from DNA to protein is not $\OneToOne$,
  multiple points in DNA space map to the same protein.
  They then confuse the many-to-one-ness of the mapping
  with a different, and not so obviously true, ``continuity" 
  property:
  small changes to DNA (often) result 
  in small changes in cell function.
  
  \item ``The theoretical structure of physics is deeply 
\emph{degenerate}.
For example, problems in classical mechanics can be approached
in multiple ways.''[p~12, emphasis mine]
This use is closer to their ``definition'',
but missing a key point. 
A better way to look at these things is to formulate 
the problem at the right level of abstraction,
and consider the choice between 
multiple representations/parameterizations of that abstraction
a computational detail.
The failure to separate a \emph{space} 
from multiple \emph{coordinate systems} for specifying
elements of that space is a common mistake 
in scientific computing.
They repeat this later in the book when they mention tensors.
(I think this may be why I never got very far 
in the first edition of SICM.)
Here's my appleal to authority on this topic::
\begin{plQuote}
{\citeAuthorYearTitle{MacLane:1954:Courses}}
{maclane:vspace}
Throughout these courses the infusion of a geometrical
point of view is of paramount importance. A vector
is geometrical; it is an element of a vector space, defined
by suitable axioms—whether the scalars be real numbers or
elements of a general field. A vector is not an n-tuple of
numbers until a coordinate system has been chosen. Any
teacher and any text book which starts with the idea that vectors
are n-tuples is committing a crime for which the proper
punishment is ridicule. The n-tuple idea is not ‘easier,’ it is
harder; it is not clearer, it is more misleading. By the same
token, linear transformations are basic and matrices are their
representations\ldots
\end{plQuote}

\item ``Engineered systems may incorporate some redundancy 
{\ldots}.
But they almost never incorporate degeneracy {\ldots},
except as a side effect of designs that are not optimal.''
Doesn't take much effort to show this is false.
Airplanes and nuclear power plants, among many other cases,
as a matter of course, incorporate many ``degenerate'' components,
where ``degenerate'' means 
``redundant distinct mechanisms for each critical task''.

\end{itemize}

At the end of this section, they refer to 
``combining partial information'' which will be covered in ch 7.
This confused discussion of ``redundancy'' and ``degeneracy''
is at best distantly related, and doesn't touch 
on the key issue: How do you combine?
Maybe ch 7 will be better {\ldots}.

\end{plSection}%{1.3 Redundancy and degeneracy}
%-----------------------------------------------------------------
\begin{plSection}{Exploratory behavior}

This section is similar to the preceding ones.
Inappropriate biological analogies that misunderstand the biology
and obscure the possibly valuable point.

The real idea is to compute something by searching a domain.
They seem to assume search must be random,
but of course that need not be so.

Figure 1.2 illustrates two possibilities:
(1) generate and test, looping until the test returns success,
and (2) generate and filter, without feedback.
I suspect that there is a better, more general picture,
related to walking around a domain,
using data accumulated on previous samples to determine
the next step.

They finally mention ``backtracking'', to be discussed 
further in ch 4 and 5.

\end{plSection}%{Exploratory behavior}
%-----------------------------------------------------------------
\begin{plSection}{1.5 The cost of flexibility}

Better than the previous sections,
because they mostly drop 
the false authority/analogy fallacies.

Starts by saying general and evolvable systems,
 that use the techniques described (very poorly!) 
 in the previous systems, are expensive.
Not clear to me that that is true, at least not always,
and nothing concrete to justify it.

``Part of the problem is that we are thinking about cost
in the wrong terms.''[p~17]
Good as far as it goes, but I think they are missing something
important.

A fundamental problem in software design, or any design problem,
is that it is hard to usefully characterize what ``better' means,
and hard to demonstrate to a skeptical observer that one
approach is ``better'' than another.
Hard enough to do with a reasonably objective observer,
and so much worse in areas like software design,
where people are heavily invested in particular technologies,
in time, money, and emotion.

I think this is a reason that so many fall into two rough classes
of fallacious reasoning. 

One is the appeal to false authority/analogy so prevalent in this 
chapter. 
In the perhaps more common version of this, the authors set 
themselves as the authority, 
usually accompanied by oracular pronouncements left to the reader
to interpret.
This book is thankfully free from that.
The next most common is to appeal to some authority/analogy
from some other problem domain, about which 
neither the reader nor the author know much.

The second class of fallacy is some variation on false 
quantification. 
In other words, pick some criterion that is
easy to measure, easy to compare, and, ideally,
provides a total ordering. 
The classic example is thinking more money is always better;
easy to tell that job A pays more than job B;
hard to tell which one really makes your life ``better''.

This chapter is essentially defending their ideas about software
design from fallacy type (2),
but they could do better.

They point out the error of premature optimization,
but miss the fact that start up costs are easier to measure
and, because they are closer in time, easier to attribute to
design choices. 
Long term maintenance costs are harder to measure and
associate with particular design choices.

The other problem with early optimization is that you don't
really understand what the code needs to do until you try to
use it in a real situation.

There is a subsection titled ``The problem with correctness''.
I agree with the conclusion, but I don't think their arguments
are very good.

``We assert this discipline [requiring proofs of correctness]
makes systems more brittle''.
But they don't offer much to support this assertion.

The fundamental problem with achieving ``correctness''
by proving a program meets a specification is:
How do you know the specification is correct?
In some sense, you are really just re-writing the program
in a different language.
At most, you are removing some distracting details,
and specifying only results, rather than how to get there.
It is, in any case, restricted to tasks  where a person can
read and verify the specification by eye.

\end{plSection}%{1.5 The cost of flexibility}
%-----------------------------------------------------------------
\end{plSection}%{Ch 1: Flexibility in nature and design}
%-----------------------------------------------------------------
\begin{plSection}{2 Domain Specific Languages}

Some misleading/confusing  (in my opinion)
vocabulary, chosen, perhaps, beacuse 
each of these things has been something of a fad
in certain subsets of the software world:
\begin{description}
\item[DSL]
As I mentioned in \cref{sec:AdditiveProgramming},
their description of ``DSL'' doesn't match 
what the term suggests to me
(and, I think, to most software developers):
restrictive syntactic sugar that reduces 
the unnecessary baroque boilerplate code. 
When it is useful, it is primarily evidence 
of a bad base language,
rather than being a good idea in its own right.

\item[Nouns and verbs]
In \cref{sec:AdditiveProgramming},
they describe a DSL as a family
of ``mix-and-match parts''.
Here, they call it  
``an abstraction in which the nouns and verbs of the language
are directly related to the problem domain.''
They don't say what they mean by ``nouns'' and ``verbs'',
or give any reference.

A quick search shows that this is a popular topic for web rants,
But I haven't found anything very insightful.
It looks like yet another misleading metaphor/simile
that lets people think they've gotten some easy insight,
when, in fact, it's just wrong.
Functions aren't verbs and objects aren't nouns,
even superficially.
And many natural languages don't  have clearly distinguished 
nouns and verbs (eg Chinese and English!).

I'm not sure where the ``noun/verb $=$ object/function'' 
metaphor originated,
maybe \citeAuthorYearTitle{Yegge:2006:Nouns}?

\item[Combinator]
This is burying the lead. 
Doesn't mean much.
Essentially just a function that takes functions as arguments,
and returns a function as value.

Real issue is functions as ``first class'' values in the 
language~\cite{wiki:FirstClassValue}.
To what extent are functions first class in Scheme?
Not fully first class in Clojure because the 
{\clojureFont clojure.lang.IFn} interface only covers applying 
a function to arguments.
You can't for example, easily find the name of a function,
determine if a function was created 
by composing other functions,
what those other functions were,
determine if a function is a closure,
what the lexical environment of the closure is,
etc.

Key properties needed in the language for ``combinators'':
\begin{itemize}
\item Functions are values that can be passed to other functions
as arguments.
\item Functions are values that can be stored (referenced)
in data structures.
\item New functions can be created at runtime, that reference
a new instance of some data structure,
usually containing functions.
\end{itemize}
\end{description}

Deserves more discussion first-class-ness and artificial barriers
between what any programmer can do and what only
the language implementer can do.

Security argument false. 
Important to be able to impose constraints on
who can read/create/execute/modify what. 
Doing this in the language definition
is neither necessary nor sufficient.
%-----------------------------------------------------------------
\begin{plSection}{2.1 Combinators}

Iterated line search optimization would provide 
a collection of better, less shallow, 
examples?

%-----------------------------------------------------------------
\begin{plSection}{2.1.1 Function combinators}

Preview: 
\begin{itemize}
  \item Use of ``combinator'' seems like a shallow allusion
  to fashionable terminology, no depth.
  \item Fail to give any convincing example of a ``combinator''
  in anything like a real application.
  \item The real idea is ``functions that manipulate functions'';
  they are missing any consideration of what that implies, 
  especially in terms of functions as first class objects.
  \item What's particularly missing is any thought about
  what a ``function'' is, eg, domain, codomain, etc.
  \item All the input/output arity stuff doesn't 
  work, certainly not in Clojure, and, I'm pretty sure, 
  not in Scheme, because functions don't have a specific input
  arity in either, Clojure doesn't have multiple value return,
  and I don't think Scheme necessarily has a fixed number of
  multiple values returned. (And the Scheme examples rely on
  features of MIT Scheme which aren't present in other dialects!)
  \item They don't even point out that multiple arity input and
  multiple value output are the same thing!
  \item A better approach would be to stop and first talk
  about what a function is: takes one element of the domain
  and returns one element of the codomain
  (if it terminates, and maybe errors are distinct from the
  codomain?).
  \item Instead of talking about multiple arguments, 
  should talk about cartesian product domains and codomains.
  This would lead nicely to a discussion 
  about value ambiguity in math, and the problems that arise
  when translating that into code. In this case,
  the problem is that math doesn't usually distinguish
  $\left(\Set{A} \times \Set{B} \right)\times \Set{C}$
  from
  $\Set{A} \times \left(\Set{B} \times \Set{C} \right)$
  or from
  $\Set{A} \times \Set{B} \times \Set{C}$,
  but {\clojureFont [[a b] c]}, {\clojureFont [a [b c]]},
  and {\clojureFont [a b c]} (to use a Clojure example)
  are different, need to be handled differently,
  and, almost surely, would be intended to mean something 
  different.
  I think many unconsciously assume cartesian product 
  is associative, but that probably doesn't carry over
  to tuples in software.
  \item The same issue comes up with {\schemeFont compose},
  and ought to be addressed.
  Would it make sense to be able to say 
  {\schemeFont (compose f0 f1 f2)}?
  Do we want to distinguish that from 
  {\schemeFont (compose f0 (compose f1 f2))}
  and {\schemeFont (compose (compose f0 f1) f2)}?
  In Clojure, {\clojureFont (comp f0 f1 \& fs)}
  is \emph{variadic} (takes as many args as you want).
  
  \item Varying input arities, and output multiple values
  are just different \emph{representations} for elements
  of the (co)domain, some syntactic sugar for common cases.
  In other words, multi-argument input and multiple value output
  should both be understood as packing and unpacking lists
  (or some other data structure) which represent elements of the
  (co)domains. 
  \item The sugar should be considered (a) typing convenience
  and (b) a hint to the compiler that the argument/returned value
  lists will be very short lived, with very limited visibility,
  and should be implemented appropriately for this restricted use.
  \item Emphasizing the need for multiple different 
  \emph{representations} of elements of the (co)domain
  would lead nicely to generic functions. 
  \item {\schemeFont compose}  and {\schemeFont parallel-combine}
  both make sense. The remaining examples are confusing,
  bug prone, and rely on obscure language features.
  They could all be done better and simpler more robustly
  as compositions with (co)domain transformations.
  \item If the section focused on {\schemeFont compose},
  that could nicely lead to something like {\schemeFont decompose},
  aka factoring of functions, something which I have found
  extremely important in many contexts, and, I think,
  generally not appreciated.
\end{itemize}

\begin{plListing}
{anonymous {\schemeFont compose} in Scheme}
{composeSchemeA}
\begin{lstlisting}[language=scheme]
(define (compose f g)
  (lambda args (f (apply g args))))
\end{lstlisting}
\end{plListing}

\begin{plListing}
{named {\schemeFont compose} in Scheme}
{composeSchemeB}
\begin{lstlisting}[language=scheme]
(define (compose f g)
  (define (the-composition . args) (f (apply g args)))
  the-composition)
\end{lstlisting}
\end{plListing}

Is {\schemeFont the-composition} in the global environment?
Given a reference to the function object, can you find its name?
The name is pointless, even for debugging,
since it doesn't tell you what was composed,
so there will be many functions named 
{\schemeFont the-composition}.
Can you determine, from an arbitrary function object,
whether it is a composition?
Can you extract the factors of a closure?
Can you get at the lexical environment of a closure?

From this point, I'm only going to include Clojure versions
of the examples.
They won't be direct translations, but rather whatever I think
makes the most sense for the topic at hand.

However, in this case, a direct translation:

\begin{plListing}
{{\clojureFont compose} in Clojure}
{compose:Clojure}
\begin{lstlisting}[language=clojure]
(defn compose [f g]
  (fn the-composition [& args]
    (f (apply g args))))
\end{lstlisting}
\end{plListing}

Note: Like Scheme, Clojure functions are not fully first class.
The {\javaFont clojure.lang.IFn} interface is barely advertised,
and basically only supports applying a function to arguments.
There is no straightforward way 
to get at a named function's name.
Best option is parsing {\clojureFont (str f)},
but there's no guarantee the output format will stay the same.
Another possibility is finding the name in the function's 
metadata, but it's also pretty murky
what metadata goes with the {\clojureFont var},
and which with the function object
(and there may even be different metadata on the 
{\clojureFont var}'s symbol).

We don't need the translation of {\schemeFont compose}, 
because the equivalent {\clojureFont core/comp}
is built in.

However, because I want to be able to do more at runtime,
I will be implementing a parallel world of more first class
functions, including {\clojureFont compose}
as a generic function (see \TODO refs to relevant sections).
Making {\clojureFont compose} generic allows,
for example, optimizations in the case where we know enough
about the internals of the factors to reduce unnecessary
intermediate pushing and popping in the argument/value stack,
shared preliminary code, etc.
See \cref{sec:Compose} for more details.

\TODO should I adopt a naming convention to distinguish
generic from non-generic functions. Or should it just be assumed
that \emph{all} functions in my code are generic?
Can this be hidden in the implementation---normal functions
overwritten with a generic one on the first occurrence
of {\clojureFont defmethod}?

Straightforward translation from Scheme:
\begin{plListing}
{translated {\clojureFont iterate} in Clojure}
{iterate:Clojure}
\begin{lstlisting}[language=clojure]
(defn iterate [^long n f]
  (if (zero? n)
    identity
    (compose f (iterate (dec n) f))))
    
(test/is (= 390625 ((iterate 3 square) 5)))
\end{lstlisting}
\end{plListing}
\TODO say something about {\clojureFont test} library.

Clojure has {\clojureFont core/iterate} 
which returns an infinite lazy sequence:
{\clojureFont [(f x) (f (f x)) \ldots]}.
Using that we get:
\begin{plListing}
{native {\clojureFont iterate} in Clojure}
{native:iterate:Clojure}
\begin{lstlisting}[language=clojure]
(defn native-iterate [^long n f]
  (fn iterated [x] 
    (last (take (inc n) (core/iterate f x)))))
    
(test/is (= 390625 ((native-iterate 3 square) 5)))
\end{lstlisting}
\end{plListing}
\NOTE the potential for off-by-one errors between the Scheme
translation and the use of {\clojureFont native-iterate}.

%-----------------------------------------------------------------
\begin{plSection}{Arity}

I'm going to skip this section.

It is mixing two concerns that would be better separated:
\begin{itemize}
  \item Patterns (``combinators")
  for combining functions into new functions.
  Something like Clojure transducers might be a source of 
  better examples.
  \item Multiple arguments and return values. 
  They are often convenient, but they greatly complicate 
  the idea of what a function is, and, especially,
  combining functions.
  
\end{itemize}
Their first examples depend on assuming that each Scheme function
has a specific fixed number of both inputs and outputs, 
which is quickly exposed as false. 
In addition, it appears that there is no standard way 
in Scheme,
to get at the input and output arities, even assuming they are 
fixed.

Another issue in this section is the lack of insight
into the difference between {\schemeFont parallel-combine}
and {\schemeFont spread-combine}. 

\end{plSection}%{Arity}
%-----------------------------------------------------------------
\begin{plSection}{Multiple values}\label{Multiple:values}

I'm going to skip this section.

Clojure doesn't have multiple value return.
Idiomatic clojure handles this, and, to some extent,
multiple input arities as well,
by packaging data into nested sequences and hashmaps,
which, as a convenience, can be parsed with destructuring
argument specifications.
See \citeAuthorTitle{Clojure:Destructuring}.

The section also fails to provide much insight 
into the meaning or lack thereof 
of allowing a function to return multiple values.

\NOTE
For symmetric functions (permuting the arguments doesn't
change the value), multiple inputs are relatively safe
and robust. 
For non-symmetric functions, 
arguments whose intended meaning is only indicated
by their position in a list
are brittle and dangerous.
As code evolves, it's common for the interface to a function
to need to support additional arguments,
and some of the old arguments become unnecessary.
If the only way to extend a function's arglist is by adding
new args to the end, you end up with something non-intuitive
and error prone.
That's directly relevant to the topic of the book,
and an example of the kind of insight that is missing.

In Clojure, this situation is commonly handle by packaging args 
into a hashmap. Not perfect, but much better 
than relying on position in the arglist alone.

\end{plSection}%{Multiple values}
%-----------------------------------------------------------------
\begin{plSection}{A small library}

I'm going to skip this section.

As they point out in exercise 2.4, 
the examples here are just various domain transformations composed
with the content functions,
obscured by multiple inputs, multiple outputs.

(\NOTE similarity 
``empty words'' 虚词 versus ``content words" 实词
in traditional standard Chinese grammar.)

\end{plSection}%{A small library}
%-----------------------------------------------------------------
\end{plSection}%{2.1.1 Function combinators}
%-----------------------------------------------------------------
\begin{plSection}{2.1.2 Combinators and body plans}

Appeal to false analogy\cite{wiki:ArgumentFromAnalogy}.

On second thought, maybe there is some accidental insight here.
The commonality of vertebrate body plans is almost surely
a consequence of evolution painting itself into a corner.

Using multiple-input multiple-output,
without any way to indicate meaning,
as the standard interface between functions
leads to brittle, painted-into-a-corner systems,
as noted in \cref{Multiple:values}.

\end{plSection}%{2.1.2 Combinators and body plans}
%-----------------------------------------------------------------
\end{plSection}%{2.1 Combinators}
%-----------------------------------------------------------------
\begin{plSection}{2.2 Regular expressions}

I'm going to skip re-doing this in Clojure, at least for now.
It seems like a bad choice of example---a mess with no useful
insights that I can see.

Standard (eg POSIX) regular expressions an example of a bad DSL.
Complaint seems to be that syntax is bad, rather than ideas.
But there very little discussion of what the ideas are.

Section proposes to improve syntax by writing a translator
of an (implied, but not discussed) better Scheme syntax
to a small subset of POSIX regexp strings.
Not clear what the point of that is supposed to be.
Would be more convincing 
if they actually implemented a pattern matcher.
Or  at least had some application that demonstrated the 
advantages of their regexp syntax over POSIX.

Missing any discussion of what the purpose of pattern matching 
might be in a more general setting.
No big ideas.
No real analysis of what's wrong with POSIX syntax.
%-----------------------------------------------------------------
\begin{plSection}{2.2.1 A regular expression combinator language}

Lists a bunch of functions 
but doesn't says what the inputs and outputs are.
Describes what they do as ``matching'' 
without any explanation of what that means.

\end{plSection}%{2.2.1 A regular expression combinator language}
%-----------------------------------------------------------------
\begin{plSection}{2.2.2 Implementation of the translator}

Skipping for now. Messy. Not clear what the point is.
Bad choice of problem doesn't offer any useful insights 
into ``combinators'' or how to go about designing
a ``combinator language''.
Don't use it for anything in the end.

\begin{plSection}{The moral of the story}

``The moral of this story is that regular expression are 
a beautiful example of how \emph{not} to build build a system.
Using composable parts and combinators to make new parts
by combining others leads to simpler and more robust
implementations.''[p 43]

The problem with this statement is that the section doesn't
provide the substance to back it up.
To do that, they would need something like
(a) some analysis of what the real problem is, with non-trivial
motivating application,
(b) a sufficiently detailed design discussion covering choices
and their resolution,
and (c) a comparison with POSIX giving evidence of where it went
wrong.

And if the big goal is ``design for flexibility'',
they need to show how their regexp syntax is more ``flexible'',
in whatever sense they want, but usually that would mean
able to handle some new problem the POSIX syntax can't.

\end{plSection}%{The moral of the story}
\end{plSection}%{2.2.2 Implementation of the translator}
%-----------------------------------------------------------------
\end{plSection}%{2.2 Regular expressions}
%-----------------------------------------------------------------
\begin{plSection}{2.3 Wrappers}\label{sec:SDFF:Wrappers}

%-----------------------------------------------------------------
\begin{plSection}{2.3 Wrappers preview}

This application is much better than the previous one.
However, I think there are significant flaws 
in how they approach it.

Issues:
\begin{itemize}
  \item The basic failing is the lack of thought in 
  problem formulation. If you want software that can be smoothly
  adapted to future changes in ``requirements'',
  then you need to abstract, generalize whatever you happen to
  thing the ``requirements'' are at present.
  And you need to have at least some specific ideas of how the
  ``requirements'' might change.
  
  \item They make the classic mistake of failing to separate
  the ``space'' and the ``coordinate system'': 
  
  The space here consists of what I will call \emph{measurables}:
  abstract quantities like length, weight, time (duration),
  temperature, 
  electric current, {\ldots}. 
  (\TODO Is there some more standard word for this?
  I see ``quantity'' used a lot, but that's seems too general and
  likely to be confusing.)
  New measurables can be constructed from base ones via 
  \emph{measurable algebra:}
  $\Pseudocode{velocity} = 
  \Pseudocode{length} / \Pseudocode{time}$.
  Note that this is a commutative group whose generators
  are the base measurables.
   
  To record a value for some measurable, we need to choose
  coordinates, aka, units of measurement, like meters, grams, 
  seconds, degrees kelvin, amperes, {\ldots}.
  
  \item What are we assuming about coordinate transformations?
  Continuous? Invertible? Affine? Linear?
  Need to be able to generate the transform for any 
  measurable expression given the transforms for the generators.
  
  \item To demonstrate the ``flexibility'' or ``robustness''
  of an approach to units of measurement, you need to, at least,
  show what's involved in 
  \begin{itemize}
    \item Adding new units for a given measurable. They get close
    to this in the discussion of 
    $\Pseudocode{fahrenheit} \leftrightarrow \Pseudocode{kelvin}$
    conversion, but it's not explicitly described in terms
    of adding new units.
    \item Adding a new measurable.
  \end{itemize} 
  And they need to at least give some argument for why their 
  approach means less work, more safety, whatever, 
  when you handle these two obvious ways in which a 
  units-of-measurement is likely to evolve over time.
  
  \item The data consists of bare numbers. It's up to the user
  to know what units of measurement a function expects,
  and what units it outputs, because there's no way to tell
  from the values themselves.
  They rely naming conventions to document what's intended
  (eg {\schemeFont celsius-to-fahrenheit}), asking for trouble.
  You can get by with that for simple conversions,
  but when you construct new ones via composition and inverse,
  watch out.
  
  \item The more subtle problem is due to the failure to reify
  measurables: nothing prevents you from converting, eg, 
  $\Pseudocode{meters}  \leftrightarrow  \Pseudocode{seconds}$.
\end{itemize}


The design tests may include:
\begin{itemize}
  \item adding a new measurable
  \item conversion to some other unit system with the
  same measurables but different units.
  \item date/time systems as additional measurables.
  Dates (yyyy-mm-dd) in particular stress the design 
  because they aren't continuous, so many conversions won't be
  invertible any more, and add lots of possibilities 
  for off-by-one fencepost errors.
  Dates also depend on location.
  \item add money/currency as a measurable. 
  This stresses the original design because the unit conversions
  are time and place (and something like conversion agent) 
  dependent.
  \item multi-dimensional measurables, particularly ones that
  are not assumed 
  elements of linear spaces, eg, position on globe as measurable,
  with {\pseudocodeFont [latitude longitude]} vs
  other spherical coordinate systems or flat map projections.
\end{itemize}

Rather than just translate this section into Clojure,
I'm also going to start over and 
try to fix the issues listed above.
To start, I'll implement SI units (see
\citeAuthorTitle{wiki:UnitOfMeasurement,wiki:SIUnits}).
See \cref{sec:UOM}.

\end{plSection}%{2.3 Wrappers preview}
%-----------------------------------------------------------------
\begin{plSection}{2.3 Wrappers initial code}
\label{sec:Wrappers:initial}

Problem posed as: 
compute the radius of a sphere of gas 
as a function of temperature,
holding pressure constant.

Not a very compelling example,
without some idea about why you might want to do this.
Without knowing ``why'', 
it's impossible to guess how the problem specification might
change in the future,
or how to generalize the problem definition
and decide what appropriate abstractions are.  

It is interesting that they write the ideal gas law 
in math notation (\cref{eq:PVnRT}),
but ignore the re-arrangement needed to get volume out:
\begin{align}
\label{eq:PVnRT}
PV \; & = \; nRT \\
& \Updownarrow & \nonumber \\
V \; & = \; \frac{nRT}{P} 
\end{align}
and don't write out the sphere volume-radius constraint 
in any form:
\begin{align}
v \, & = \, \frac{4\pi}{3} r^{3} \\  
& \Updownarrow & \nonumber \\
r \, & = \, \left[ \frac{3}{4\pi} v\right]^{\frac{1}{3}}
\end{align}
They don't say anything about equations vs procedures
and, in particular, representing equations in code.
The failure to reify equations leads to an implicit
assumption---that conversion procedures 
(and the procedures extracted from the gas law and sphere volume
formula) are \emph{invertible}.

In fact, at least one of these, the sphere radius from volume
function, isn't invertible, and that fact is hidden by the
failure to consider what numbers are being used.
They mention explicitly that the {\schemeFont 4/3} 
and {\schemeFont 1/3}
are rational numbers ($\Space{Q}$), implying, but not stating,
that there is some arbitrary precision implementation of
$\Space{Q}$ in Scheme.
But {\schemeFont (define pi (* 4 (atan 1 1)))}
and {\schemeFont (expt $x$ 1/3)}
can only be an approximation---unless Scheme includes some version
of computable $\Space{R}$.
(See \citeAuthorYearTitle{Bridger:2019,Henle:2012:RealNumbers}.)

Clojure translations:
Note that I am using {\javaFont double} for all the numbers,
rather than ratios for some of the numbers,
as in the original Scheme version.
A closer translation might use {\clojureFont clojure.lang.Ratio}
for, eg, {\clojureFont 4/3}
However, though I haven't checked for several years,
I have, in the past, found enough problems in the 
{\clojureFont Ratio} implementation to distrust it
(eg {\clojureFont double -> Ratio -> double} is \emph{not}
an identity transformation).
All the other Java or Clojure rational number implementations
I have tried have been broken, except for 
\citeAuthorTitle{Occil:2021:NumbersJava}.
I may try that or my own implementation 
of computable $\Space{R}$ some time in the future.
\begin{plListing}
{Gas law and sphere radius}
{Gas:law}
\begin{lstlisting}[language=clojure]
(def gas-constant "J/(K*mol)" (double 8.3144621))

(defn gas-law-volume ^double [^double pressure 
                              ^double temperature 
                              ^double amount]
  (/ (* gas-constant amount temperature)
     pressure))

(def one-third (/ 1.0 3.0))
(def three-over-4PI (/ 3.0 (* 4 Math/PI)))

(defn sphere-radius ^double [^double volume]
  (Math/pow (* three-over-4PI volume) one-third)) 
  
(sphere-radius (/ (* 4 Math/PI) 3)) -> 1.0
\end{lstlisting}
\end{plListing}

\NOTE {\schemeFont make-unit-conversion} seems to be missing
in the book.

A quick and dirty Clojure closure
implementation follows.

\TODO Probably a {\clojureFont deftype}
implementing {\clojureFont clojure.lang.IFn}
and some {\clojureFont Invertible} interface would be better.
A minimally reasonable version would just be an 
{\clojureFont Invertible}
function, nothing about "unit converters" required.
An {\clojureFont Invertible} function interface would 
have an explicit domain and codomain, 
at least one (pseudo-random) generator for the domain and
an equivalence relation for the codomain, to used used in
testing whether the 'inverse' really is one.
 
\begin{plListing}
{First pass at invertible converters}
{First:invertible:converter}
\begin{lstlisting}[language=clojure]
(defn make-converter ^IFn [^IFn$DD f ^IFn$DD f-inverse]
  (fn "zero arity call returns inverse converter."
    (^IFn []  (make-converter f-inverse f))
    (^double [^double x] (f x))))

(defn invert ^IFn [^IFn converter] (converter))
\end{lstlisting}
\end{plListing}

\NOTE this is the affine conversion for absolute temperature.
The book does not consider the difference between (affine) 
absolute temperature and (linear) change in temperature.
Could do these as general 'affine' 
{\javaFont double} $\rightarrow$ {\javaFont double} functions,
with inverse (and linear derivative) generated automatically.
Could also implement with some {\clojureFont Rational}
 or {\clojureFont RationalFloat}
for exact results.

\TODO Is there some general principle that would suggest which
rearrangements of these float expressions would give 
the most accurate results? In particular, which would make
the inverses closer to real inverses, even if the forward
transformation isn't perfect? 

\begin{plListing}
{First pass at 
fahrenheit $\leftrightarrow$ celsius $\leftrightarrow$ kelvin}
{First:fahrenheit:celsius}
\begin{lstlisting}[language=clojure]
(def ^IFn fahrenheit-to-celsius
  (make-converter 
    (fn ^double [^double f] (/ (* 5 (- f 32)) 9))
    (fn ^double [^double c] (+ (/ (* 9 c) 5) 32))))

(def ^IFn celsius-to-kelvin
  (let [zero-celsius 273.15] ; kelvin degrees
    (make-converter 
      (fn ^double [^double c] (+ c zero-celsius))
      (fn ^double [^double k] (- k zero-celsius)))))

(fahrenheit-to-celsius -40) -> -40.0
(fahrenheit-to-celsius 32) -> 0.0
((invert fahrenheit-to-celsius) 20) -> 68.0
((comp celsius-to-kelvin fahrenheit-to-celsius) 80)
-> 299.81666666666666
\end{lstlisting}
\end{plListing}

Finally the radius calculation, 
still with no indication why anyone would want to do this,
which means you can't reasonably for future variations.

Note that {\schemeFont psi-to-nsm} is not implemented in the 
text as an invertible converter (did they just get tired?).

\begin{plListing}
{First pass at radius calculation}
{First:radius}
\begin{lstlisting}[language=clojure]
(def ^IFn pound-to-newton
  (make-converter 
    (fn ^double [^double pounds] (/ pounds 0.22480894387))
    (fn ^double [^double newtons] (* newtons 0.22480894387))))

(def ^IFn inch-to-meter
  (make-converter 
    (fn ^double [^double inches] (/ inches 39.3700787))
    (fn ^double [^double meters] (* meters 39.3700787))))

(def ^IFn psi-to-nsm
  (make-converter 
    (comp pound-to-newton
          (invert inch-to-meter)
          (invert inch-to-meter))
    (comp inch-to-meter 
          inch-to-meter 
          (invert pound-to-newton))))

((invert inch-to-meter)
    (sphere-radius
      (gas-law-volume
        (psi-to-nsm 14.7)
        ((comp celsius-to-kelvin fahrenheit-to-celsius) 68)
        1)))
-> 7.049624642536843
\end{lstlisting}
\end{plListing}

\TODO my answer only agrees with the text in the first 8 out of
15 digits. 
Odds are different constants in the {\schemeFont pound-to-newton}
and {\schemeFont inch-to-meter} converters, 
but it could also be the use of rationals in the Scheme code.
Not really worth tracking down, but a good example
of floating point and approximate data input issues.
\end{plSection}%{2.3 Wrappers initial code}
%-----------------------------------------------------------------
\begin{plSection}{2.3.1 Specialization wrappers}

Poor choice of words, I think.
Neither ``specialization'' nor ``wrapper'' seem to fit.

What they are really doing is a standard coordinate transform:
\begin{equation}\label{eq:CoordTranform}
f_{1} = g^{-1} \circ f_{0} \circ g
\end{equation}
where $f_{0}$ computes something of interest,
in a given coordinate system $C_{0}$,
for both $f_{0}$'s domain and codomain.
If we want to compute the same thing,
but use different coordinates, $C_{1}$, for input and output,
we need an invertible $g : C_{1} \rightarrow C_{0}$,
and we get the equivalent computation via the double
composition in \cref{eq:CoordTranform}.

There are some interesting issues here the difference between
the domain and codomain as \emph{sets}
and the possibility of multiple \emph{representations}
for the elements of those sets, 
with coordinate systems being just one way
in which representations might vary.
If we think about it that way,
then coordinate transformations are more-or-less a special case
of method generation/lookup.
In the more general case, a different algorithm might be
used for different coordinate systems,
so there really is a different method
not one that can generated trivially via composition.

It's easy to get confused. 
Sometimes the sets in question
are sets of representation instances of a certain kind,
so there is no distinction between the elements 
and their representations.
There is only one representation; 
the element is its representation; etc.

The point of this section is generating the coordinate transforms
automatically from a set of base unit transforms,
a specification for how the coordinates are constructed
from the base units,
and specifications for the expected and provided coordinates.

Missed opportunities: 
\begin{itemize}
  \item They don't distinguish the ``quantity''
  ({\pseudocodeFont length}, {\pseudocodeFont time}, 
  {\pseudocodeFont mass}, {\ldots}) being measured
  from the units used to record the measurement.
  Easy to accidentally convert inches to kilograms.

  \item the possible ``quantities' to be measured are 
  a commutative group generated from the base quantities.
  For example, {\pseudocodeFont length} and 
  {\pseudocodeFont time} are base quantities.
  The group operation is just symbolic multiplication:
  \begin{equation}
  \text{\pseudocodeFont area} = 
  \text{\pseudocodeFont length} \ast \text{\pseudocodeFont length} 
  = \text{\pseudocodeFont length}^2 
  \end{equation}
  The group inverse is symbolic division:
  \begin{equation}
  \text{\pseudocodeFont frequency} = 
  \frac{1}{\text{\pseudocodeFont time}} 
  \end{equation}
  and, for example,
  \begin{equation}
  \text{\pseudocodeFont acceleration} = 
  \frac{\text{\pseudocodeFont length}}
       {\text{\pseudocodeFont time}^2} 
  \end{equation}
  
  \item The domains and codomains of the kind of
  chemistry/physics calculations are constructed from the
  ``quantity'' group. Note that these domains may refer to a
  given quantity multiple times. For example, position in space
  will have 3 {\pseudocodeFont length} quantities 
  
  \TODO this isn't right, 3d ``length" isn't
  really the same as 3 lengths.
  More like 
  $\text{\pseudocodeFont length} \times 
  \text{\pseudocodeFont length} \times 
  \text{\pseudocodeFont length}$.
  Then spacetime would $(\times^{3} \text{\pseudocodeFont length})
  \times \text{\pseudocodeFont time}$?
  Is this a ring rather than a group?
  We also have the issue of affine quantities,
  eg 
  {\pseudocodeFont instant-in-time} vs
  {\pseudocodeFont change-in-time},
  {\pseudocodeFont temperature} vs
  {\pseudocodeFont change-in-temperature}
    
\end{itemize}

I'm not showing any Clojure translations here, since they 
require functions defined later.

\end{plSection}%{2.3.1 Specialization wrappers}
%-----------------------------------------------------------------
\begin{plSection}{2.3.2 Implementing specializers}
%-----------------------------------------------------------------
\begin{plSection}{Group operations}

The converters form a commutative group.

The group operation is essentially composition
(note the reversed order of arguments relative to 
{\clojureFont comp}).
The group inverse operation is 
implemented with {\clojureFont invert}
in \cref{listing:First:invertible:converter}.
As in the text, we add 2 convenience operations:
{\clojureFont divide} and {\clojureFont pow}:
\begin{plListing}
{Converter group}
{converter:group}
\begin{lstlisting}[language=clojure]
(defn * ^IFn [& us]
  (make-converter (apply comp (reverse us))
                  (apply comp (map invert us))))

(defn divide ^IFn [^IFn u0 ^IFn u1] (* u0 (invert u1)))

(defn pow ^IFn [^IFn u ^long n]
  (cond (zero? n) u
        (neg? n) (invert (pow u (- n)))
        :else (* u (pow u (dec n)))))
\end{lstlisting}
\end{plListing}
\TODO a more efficient, tail=recursive version 
of {\clojureFont pow}.

\end{plSection}%{Group operations}
%-----------------------------------------------------------------
\begin{plSection}{Memoizing converters}

A two key map in-unit, out-unit -> converter.
This is easy in idiomatic Clojure,
but a better implementation would use a general purpose
memoize package, rather than this one-off version.

\TODO do we still need the converter to hold its inverse?

\TODO How should {\clojureFont register} handle an already
registered converter?

\NOTE how {clojureFont get-converter} handles the identity case.
Is this the best way to do that?

\begin{plListing}
{Memoized converters}
{Memoized:converters}
\begin{lstlisting}[language=clojure]
(def ^:private converters (atom {}))

(defn register [from to ^IFn converter]
  (swap! converters assoc-in [from to] converter)
  (swap! converters assoc-in [to from] (invert converter)))

(defn get-converter ^IFn [from to]
  (if (= from to)
    identity
    (let [c (get-in @converters [from to])]
      (assert (not (nil? c)) [from to])
      c)))

(register 
  :fahrenheit :celsius 
  (make-converter 
    (fn ^double [^double f] (/ (core/* 5 (- f 32)) 9))
    (fn ^double [^double c] (+ (/ (core/* 9 c) 5) 32))))
(register 
  :celsius :kelvin
  (let [zero-celsius 273.15] ; kelvin degrees
    (make-converter 
      (fn ^double [^double c] (+ c zero-celsius))
      (fn ^double [^double k] (- k zero-celsius)))))
(register
  :pound :newton
  (make-converter 
    (fn ^double [^double pounds] (/ pounds 0.22480894387))
    (fn ^double [^double newtons] (core/* newtons 0.22480894387))))
(register
  :inch :meter
  (make-converter 
    (fn ^double [^double inches] (/ inches 39.3700787))
    (fn ^double [^double meters] (core/* meters 39.3700787))))
(register
  :fahrenheit :kelvin 
  (* (get-converter :fahrenheit :celsius) 
     (get-converter :celsius :kelvin)))
\end{lstlisting}
\end{plListing}
\end{plSection}%{Memoizing converters}
%-----------------------------------------------------------------
\begin{plSection}{Monomial ratios}

SDFF represents the non-base elements of the converter group
as quoted expressions. This creates the issue what to do with
different, but equivalent expressions. This may be covered 
later with an evaluator for expressions returning the reduced
form.
 
Here, I think it's better to stop and think a bit.
A non-base converter, as defined here, is a monomial ratio:
essentially a monomial
with both positive and negative powers.
(Is the a standard name for this?)

In any case, I think it's safer to use a representations
of the monomial ratios as the key into the converter table.
An easy idiomatic Clojure approach to this is to use a hashmap
with unit keywords as keys/variables and 
long values as the exponents. 

Modest improvements to {\clojureFont get-converter}
 could eliminate the need
to define and register the monomial ratio cases.
Would be easy enough to generate them, assuming the base
converters have been registered. 
However, I'm going to skip that for now.
The effort would be better spent on an implementation
that fixes the conceptual problems, such as reifying
measurable quantity group (length, time, etc.)
and organizing the converters around that.
If nothing else, I'd want to replace the one-off parts
of this with general parts (eg invertible functions
with access to (co)domain sets, algebraic structures, etc)

\begin{plListing}
{Non-base converter as a monomial ratio}
{monomial:ratio}
\begin{lstlisting}[language=clojure]
(register 
  {:pound 1 :inch -2}
  {:newton 1 :meter -2}
  (make-converter (comp (get-converter :pound :newton)
                        (get-converter :meter :inch)
                        (get-converter :meter :inch))
                  (comp (get-converter :inch :meter) 
                        (get-converter :inch :meter) 
                        (get-converter :newton :pound))))
\end{lstlisting}
\end{plListing}
\end{plSection}%{Monomial ratios}
%-----------------------------------------------------------------

Finally, we can implement {\clojureFont specializer}:

\begin{plListing}
{Specializer}
{Specializer}
\begin{lstlisting}[language=clojure]
(defn specialize ^IFn [^IFn procedure
                        implicit-output-unit
                        & implicit-input-units]
  (fn specializer ^IFn [specific-output-unit
                        & specific-input-units]
    (let [output-converter (get-converter implicit-output-unit
                                          specific-output-unit)
          input-converters (mapv get-converter 
                                 specific-input-units
                                 implicit-input-units)]
      (fn specialized-procedure ^IFn [& arguments]
        (assert (= (count input-converters) (count arguments)))
        (output-converter
          (apply 
            procedure
            (map (fn [converter argument] (converter argument))
                 input-converters
                 arguments)))))))
\end{lstlisting}
\end{plListing}

And test it:
\begin{plListing}
{Specializer:test}
{Specializer:test}
\begin{lstlisting}[language=clojure]
(def glsr-SI (comp sphere-radius gas-law-volume))
(def glsr-specializer 
  (specialize glsr-SI :meter {:newton 1 :meter -2} :kelvin :mole))
(def glsr-US 
  (glsr-specializer :inch {:pound 1 :inch -2} :fahrenheit :mole))
(glsr-US 14.7 68 1) -> 7.049624642536843
\end{lstlisting}
\end{plListing}
which is the same result as in \cref{listing:First:radius}.

\end{plSection}%{2.3.2 Implementing specializers}
%-----------------------------------------------------------------
\begin{plSection}{2.3.3 Adapters}

``This is an important principle:
rather than rewriting a program to adapt it to a new purpose,
it's preferable to start with a simple and general base program
and wrap it to specialize for a particular purpose.
The program doesn't know anything about the wrappers,
and the wrappers make few assumptions about the underlying 
program.''

I don't buy this.
At best, this section uses a poorly chosen application
that doesn't make their point.

The {\clojureFont gas-law-volume} and {\clojureFont sphere-radius}
functions are not ``simple and general''.
The ``wrapper' aspect is essentially just composition.

The meat of the section is the units conversion algebra,
which potentially could offer a lot as an example of how
to think thru a problem domain,
and separate the key abstractions 
from details of particular representations.
The choice of coordinate system is a specific, relatively easy
to understand example where one wants support for alternate 
representations of the same abstract entity.

\end{plSection}%{2.3.3 Adapters}
%-----------------------------------------------------------------
\end{plSection}%{2.3 Wrappers}
%----------------------------------------------------------------
\begin{plSection}{Aside}
\NOTE 
I should have checked the book's appendices more carefully.
It is only at this point that I discovered
\citeAuthorYearTitle{HansonSussman:2021:SDFFSource}
mentioned in 
\citeAuthorYearTitle[Appendix A]{HansonSussman:2021:SDFF}.
At some point I may go back and compare my code, 
but for now I'll just refer to the provided code going forwards.
\end{plSection}%{Aside}
%-----------------------------------------------------------------
\begin{plSection}{2.4 Abstracting a domain}

``{\ldots} a domain specific language layer can be created
as a basis for software about board games.''

I think boards games is a pretty good example.
Simple enough, but also not unrealistic.
Well understood by most readers to support non-trivial
consideration of alternate problem formulations
without requiring many pages of background.

Not happy about ``domain specific language'',
and, especially, ``layer'',
which I think are at best poor choices of words,
and likely misleading for most.

I think the analysis of the problem is lacking.
They jump too quickly from talking about ``abstracting a domain''
for all board games, 
to just implementing 
``a referee for checkers that will compute all legal moves 
for a player at a given state of play''.
I think this is exactly how you get into trouble.
Without some clear ideas of how that referee might be used
you are likely to make unconscious assumptions.
In this case, their specification of what a ``referee'' is
seems obviously wrong.
Though a board game needs some way to validate legal moves, 
that doesn't require generating a list of all of them, 
and it seems reasonably likely that approaching it that way
could lead to something unusable. 

There are many obvious questions they should have considered,
even if just eliminating some options out of hand,
to keep the example to a feasible size.
For example:
\begin{itemize}
  \item How are the board game implementations meant to be used?
  \item In what ways are board games allowed to vary?
  3d chess? Hexagonal grids? Continuous position and movement?
  General graphs (eg Risk, Diplomacy,Clue)? 
  What about Monopoly like games?
  \item Randomization?
  \item How many players? 
  Black, white, red, green, blue, {\ldots}
  Teams (multiple players share eg the black pieces)?
  \item Are there going to be human players? 
  \item Are there algorithm players?
  \item What is the ``player'' API?
  \item Do you want to be able to provide hints to human players?
  Look-ahead for checkmate, etc.?
  \item What kind of UI?
  I suspect a graphical UI, or at least a board display,
  would make this more appealing to students, and make it easier
  to discover problems.
  \item What about turn-based vs concurrent moves?
  \item What about time-limited moves (eg blitz chess)?
  \item What does each player get to know about the current 
  position (eg battleship-like games)?
  \item What are the speed requirements?
  \item How about space? 
  \item Do you need history and undo?
  \item Scoring and accumulating points, with game play
  affected by scores? 
\end{itemize}

Even if they want to focus on move validation,
to keep the example small, it would be better to provide
an enclosing, working game framework in which to embed it.
But the most value would come from considering,
in reasonable depth,
how a game framework could be structured.

%-----------------------------------------------------------------
\begin{plSection}{2.4.1 A monolithic implementation}

I think starting with ``A monolithic implementation'' 
is probably a mistake.
A lot of code to wade thru only to be told it's the wrong way
to do thing.

A better strategy might be to do two reasonable implementations,
one of checkers, one of chess, or maybe go.
Then look and see what could be shared and 
what is specific to each.
And, most important, reflect on how that illuminates 
what the structure of a game problem domain might be.

Of course, the big failing here is defining the task as 
implementing
``a referee {\ldots} that will compute all legal moves'',
which doesn't need deep thought to see that it makes no sense
as a part of a game implementation.
In fact, their approach would make a good example of how
people end up ``programming themselves into a corner''
by following a bottom-up approach,
seizing on the first task you can think of, 
without fully considering the big picture.

Another complaint: I am noticing consistently bad naming,
both in the English text and the code.
I missed the fact that the ``referee'' was in fact a move
generator on first read, because ``referee'' sounds
like ``move validator'', which would actually make sense,
and not like ``move generator'', which doesn't.

I'm not going to translate the ``monolithic implementation''
into Clojure---what would be the point?
But it is filled with a number of other, unexamined
bad choices, that aren't
addressed in the ``factored'' version.
I want to comment on them and point out better options,
and then do a decent alternative to the factored version
(though I will probably just do a move validator).

%-----------------------------------------------------------------
\begin{plSection}{A checkers domain model}

They assume, without justification, a domain model based
on 3 ``abstract types'': 
{\schemeFont board}, {\schemeFont piece},
and position $\defeq$ {\schemeFont coords}.
They provide a (poorly specified) API for 
{\schemeFont board} and {\schemeFont piece},
but nothing separately for {\schemeFont coords}.
The {\schemeFont board} API refers to {\schemeFont coords},
so it's really a joint API.
There is some vague discussion about 
absolute vs relative coordinates,
vs directions, but
``We won't define the procedures for manipulating coordinates;
they should be self-explanatory.''
(My ``painting yourself into a corner thru unexamined 
assumptions'' alarm is going 
WHOOP! WHOOP! WHOOP!.)
It's reasonable to not show the code here,
but they could at least refer to the book's code link,
and give the details there.
However, as it is, the associated scheme code it almost entirely
comment and documentation-free, providing another example
of how not to do things.

They refer to a ``player'' and ``color'', but don't reify them.
There is apparently a mutable ``current player'' 
and many of the {\schemeFont board} and {\schemeFont piece}
API functions depend on who the ``current player'' is,
but nothing to access that (let alone do it safely under 
concurrent operations).

The mis-named ``referee" task is defined as generating all legal
moves, but moves are not reified!
Probably a ``move'' should be anything that changes the
``state of play'', 
which is also clearly essential and also not reified,
instead it is dispersed and hidden in the board and pieces.

They are missing something that is probably more important 
than anything in the book:
One of the first things to do when designing for robustness and
flexibility is to consider what state you need,
how that state changes over time, 
and how to partition it to minimize dependencies on changes.
(Note that state changing over tine doesn't necessarily require
mutable data; growing immutable data is also a change.)

%-----------------------------------------------------------------
\begin{plSection}{Comments on monolithic ``board'' API}

Poorly chosen, misleading names.

Really a joint {\schemeFont coords} $\times$ {\schemeFont board}
interface.

Redundant: Seven functions in total, 
but five could be collapsed into the single function, 
{\schemeFont (board-get {\itshape coords board})},
if the API for the returned {\schemeFont piece} included
an obvious {\schemeFont piece-owner} function.
 
The remaining functions, 
{\schemeFont (current-pieces {\itshape board})} and 
{\schemeFont (is-position-on-board? {\itshape coords board})},
would be unnecessary if we had a straightforward way to iterate
over all valid values of {\schemeFont coords},
though we would probably want some convenient way to iterate
over \emph{all} {\schemeFont pieces}.
(Best to iterate over all pieces and interrogate whether each is 
on the board or not---some games might permit moving pieces
back to the board.)

The biggest problem with the {\schemeFont board} API is 
the unconsidered assumption that it is a rectangular grid.
This make a really good example of how lack of a little thought
paints the resulting system into a corner!
Even a brief glance at a few relevant Wikipedia pages
(eg \citeAuthorTitle{wiki:Checkers,wiki:ChessVariants})
would make it clear how bad this choice is.
I actually think most SDEs, if given time to consider
a handful of different games, would see that a directed graph
is both simpler and a more general way to implement both the game  
``board'' and the sets of possible moves.

One good thing about the {\schemeFont board} API is that
it doesn't assume much that is specific to checkers alone;
it probably works for all (?) games played on rectangular grids,
and seems like it would support chess without changes.

\end{plSection}%{Comments on monolithic ``board'' API}
%-----------------------------------------------------------------
\begin{plSection}{Comments on monolithic ``piece'' API}
\label{sec:monolithic:piece}

{\schemeFont piece-coords} is bad naming.
({\schemeFont piece-position}, for example,
wouldn't imply a rectangular grid),
but we probably need piece$\rightarrow$position
and position$\rightarrow$piece functions, whatever a ``position''
is.

{\schemeFont should-be-crowned?} seems like a strawman,
obviously checkers-only, and only for variants were crowning is
required. 
Probably crowning should be considered a move,
like anything that changes the state-of-play;
some moves may be required, depending on the game rules.

{\schemeFont (crown-piece {\itshape piece})} is misleading.
The description says that it just returns a ``king'',
which wouldn't be very useful.
One has to guess that it really means creating a new ``king''
piece belonging to the same owner as the
{\schemeFont {\itshape piece}} argument,
removing the {\schemeFont {\itshape piece}} from its current
location and putting the ``king'' there.

{\schemeFont (possible-directions {\itshape piece})}
is also badly named, particularly since there is nothing
about what a ``direction'' is and how or even whether it differs
from a ``move''.
The description is very confusing:
``directions a piece may consider for a move''
but 
``not {\ldots} whether moving in that direction is permissible.''

\end{plSection}%{Comments on monolithic ``piece'' API}
%-----------------------------------------------------------------
\end{plSection}%{A checkers domain model}
%-----------------------------------------------------------------
\begin{plSection}{A checkers referee}

``We need a data structure to represent each move.''
Described as a ``path'' which is a list of ``steps'',
in reverse order.

They justify ``reverse order'' because that makes it easier
to share partial paths, which is only true because they are 
implicitly assuming the use of {\schemeFont car/cdr} linked lists.
To the extent that there is an idea here, it's that the set of
possible paths form a tree. 
But using {\schemeFont car/cdr} linked lists for this gives you
trees that are only good for iterating from leaf to root,
the wrong direction for most uses.
And the set of paths actually form a directed graph, not a tree,
because multiple partial paths can lead to the same intermediate
state (eg [left, forward, right] is the same as 
[right, forward, left] is the same as [forward],
in some rectangular game where all 8 directions are allowed).
 
This is an excellent example of 
``programming yourself into a corner'' 
thru premature optimization.
Perhaps this is meant as a strawman to be criticized later?
Remains to be seen.

{\schemeFont (step-to {\itshape step})}\\
makes no sense.
It is supposed to return some piece, but which piece?
Why would stepping a piece change it, and if it isn't changed,
why would you need to return it?

{\schemeFont (step-board {\itshape{step}})}\\
is worse.
It must be making a reference to a globally visible 
{\schemeFont board}, and seems to assume that global board
is mutated by the step.
It's also returning a board, and if that's a new board,
how do you control which board gets stepped next?

Worst of all, it appears that you have to make 2 calls
to execute a step---one on the piece, and one on the board.
What happens if you only make one of these?

{\schemeFont (make-simple-move {\itshape coords piece board})}\\
creates a step; why isn't it called {\schemeFont make-step}?

This appears to be name leakage from the implementation
in \cref{sec:FactoringCheckers}---an indication of the lack 
of critical editing or serious proofreading.
I don't think much of the book would have survived
even a cursory code review of the kind typically required
in industrial software development.

The description implies the step moves the piece to the
absolute position {\schemeFont {\itshape coords}},
and the returned object contains references to the 
{\schemeFont {\itshape coords}},
{\schemeFont {\itshape piece}},
and {\schemeFont {\itshape board}}.
It seems like passing in a change in position 
would make more sense;
in any case, you will need the current position to tell
whether the move is legal.

{\schemeFont 
(make-jump {\itshape new-coords jumped-coords piece board})}:\\
the two coordinate arguments are redundant.
Both are determined by the direction of the step,
whether you want to consider it a two square step to the final
position, or a one square step, followed by a ``capture'' step,
which forces another one square step in the same direction.
Almost surely better break down a jump into 3 steps in that way
(would be necessary for chess for example)
and separate out the notion that certain steps 
force the next step.

{\schemeFont (replace-piece {\itshape new-piece old-piece board})}:\\
They describe this as a ``step'', 
which expands the concept of ``step'',
probably in a good way, but it exposes their failure to consider 
what they mean by ``step'' in the first place.
Almost better to break this up into {\schemeFont add-piece}
and {\schemeFont remove-piece}.
In any case, 
we need the missing{\schemeFont remove-piece} for jumping.

{\schemeFont (path-contains-jumps? {\itshape path})}:\\
Not very useful without any API for paths.
Simpler and more flexible if we 
just test for {\schemeFont remove-piece}
steps in a path, and dispense with ``jump''
as a separate step type.

Given the problems with the APIs, I don't think it's worth
the move generation code in more detail.

\end{plSection}%{A checkers referee}
%-----------------------------------------------------------------
\begin{plSection}{Critique}

For a ``critique'', this is pretty self-congratulatory.
The only problem they recognize is the fact that the rules
of checkers are distributed throughout the code,
but nothing about how this may be a consequence 
of bad problem formulation.

\end{plSection}%{Critique}
%-----------------------------------------------------------------
\end{plSection}%{2.4.1 A monolithic implementation}
%-----------------------------------------------------------------
\begin{plSection}{2.4.2 Factoring out the domain}
\label{sec:FactoringCheckers}

Another abuse of English: should be ``Factoring the domain''.
``Factoring out'' would mean removing the domain,
or at least redoing the code so that any domain can be plugged in
to whatever is left without it.
The point here is actually factoring out the game rules and 
from the domain and play execution, so that other rules can
be plugged in.

``Can we separate the domain model and control structure
from the rules of checkers?''

This is doomed to failure, due to the lack of thought
in problem formulation.

What are ``rules''? 
Unspecified.
Is that the right way to think about it? 
Never consider the question, let alone any alternatives.

The use of the word ``rules'' here is similar
to their use of ``combinator'', ``domain specific language'',
``wrappers", ``adaptors",``layers'', {\ldots};
terms that are fashionable/faddish concepts in certain
ares of software development, eg, ``rule-based systems''.
They grab these buzzwords, use them to mean something that
doesn't match the original concept, presumably because they
don't stop to consider exactly what either the original 
or their version means in any depth, and, most important,
don't stop to consider whether either version is actually a good
idea for the problem at hand. 


%-----------------------------------------------------------------
\begin{plSection}{A domain model}

``we can reuse the coordinates, pieces, and board from our
monolithic implementation {\ldots}.''
So most of the problems won't be addressed.

They first introduce the idea of a {\schemeFont piece-type}.
This should be been an obvious part of the ``monolithic''
implementation. 
They don't clearly define the scope of the
problem, but they do at least mention the idea that 
one might want to do both checkers and chess.

In fact, this was implied by the ``monolithic'' piece API.
What does {\schemeFont crown-piece} do, if it isn't replacing
the original piece with a ``king''?
See \cref{sec:monolithic:piece}.

Missed opportunity:
For checkers alone, piece types aren't really necessary.
It could be represented in the same way as a in physical checkers,
by having two pieces in the same position.
This could have been a simple example of working thru a design
choice: for checkers (and go and probably others)
implementations the simplicity
of all pieces being the same could be an advantage,
but it fails if you want to extend to chess.

Two function API:
{\schemeFont (piece-type {\itshape piece})}
and {\schemeFont (piece-new-type {\itshape piece type})}.

No indication of what a {\schemeFont {\itshape type}} might be,
so, strictly speaking, you can't do anything with it!

{\schemeFont piece-new-type} is supposed to return 
``a new piece identical to'' its argument, except for the 
 {\schemeFont {\itshape type}}.
This second function is hiding lots of problematic assumptions
about what attributes belong to a piece, what it mean for 2 pieces
to be identical. 
I think they are assuming ``identical'' includes being at the
same position, but I don't think many would see changing the
location of a piece as returning a new, different piece,
or even modifying the state of a given piece.

A major part of problem formulation is deciding which components
own which parts of the state.
You could, in principle, argue for a design in which the pieces
own the location information. 
It's hard to see how that could be a good choice---when you need 
to know what piece is in a given location, you would have 
ask all the pieces where they are.
Regardless, this is a key decision that needs explicit
thought and the choice expressed clearly---another missed
opportunity for demonstrating good practices in problem
formulation.

They say something about the crowning API no longer
being ``part of the core domain model'', which is confusing
because they never said what ``the core domain model'' is,
and they describe the crowning functions as part of the piece API
to begin with.

The ``monolithic'' implementation had a fairly incoherent
and incompletely specified
API for ``steps'', ``jumps'', and ``moves''.
Here they replace that with a similarly incoherent
and unspecified API for ``changes'' and ``partial moves''.

{\schemeFont (make-change {\itshape board piece flags})}:

An API with arguments named {\schemeFont {\itshape flags}}
is one of the classic bad code smells.
(``Code smells are usually not bugs; 
they are not technically incorrect and 
do not prevent the program from functioning. 
Instead, they indicate weaknesses in design 
that may slow down development 
or increase the risk of bugs or failures 
in the future."~\cite{wiki:CodeSmell})

We have piece types, why not change types?

``We will replace the path idea with a more abstract notion 
called a \emph{partial move.}''
In fact, there is no difference.
And there is also nothing inherently ``partial'' about it either.

In other words, how is a ``partial move'' different from a 
``move''?
Answer: it isn't. 

How is a ``partial move'' containing one ``move'' different from a 
``change''? It isn't.

Isn't an empty ``partial move'' the same thing as a
``no change'' change? 

Missed opportunity: Doesn't this sound a lot like function
composition?

It wouldn't have taken much to start with a general picture
of a game built from a set of players, a state-of-play,
and a set of possible actions that a player can invoke. 
Actions are really just functions that take the state-of-play
and return an updated state. 
And actions can be composed just like functions.

I'm going to skip over the rest of the 
{\schemeFont \itshape pmove} API, 
since it is similarly bad,
and unnecessary once you realize ``steps'',
``jumps'', ``crowning'', ``paths'', ``changes'',
``partial moves'', ``moves'', ``captures'', {\ldots}
 are all the same thing:
first class functions with what ought to be a standard API
for getting at the (co)domain and other constraints on allowed
composition, and
the factors of a composed function,
etc.

\end{plSection}%{A domain model}
%-----------------------------------------------------------------
\begin{plSection}{An executive}

This is backwards. 
The section primarily describes a vague notion 
of two very different kinds of 
``rules'': 
\emph{evolution rules} 
used to build moves by extending the moves in an existing set,
and \emph{aggregate rules} that are used to filter sets of moves.

This is very different from what ``executive'' usually means,
which has to do with controlling move get executed,
maintaining the actual state-of-play under possibly
concurrent moves,
as opposed to speculative states-of-play that might be created
in the process of a player choosing its next move.

There are two issues: handling updates to the real state-of-play,
and making sure that players' moves are legal with respect to
the current state.
These are largely orthogonal concerns 
and ought to be kept separate.

The code in the section is still based on the nonsensical,
and unstated, assumption that the only way to proceed is 
to generate all possible moves.
In particular, it does that by recursively calling
{\schemeFont evolve-pmove} to generate some large set of moves
that are only filtered for legality at the end.

Problems:

Wouldn't it be more efficient to filter as you go?

Even though this isn't a normal ``rule-based system'', it has the
same basic fault: how do check that the rules are correct?
In particular , how do you check that the evolution rules 
are actually generating at least all legal moves?
And that the aggregate rules aren't removing valid moves
or allowing illegal ones thru?

My guess is that correctness is easier to achieve 
when you isolate and minimize the amount of data you need
to determine correctness at each local decision.
For example, explicitly encoding the board topology
into a directed graph (or a few graphs depending on piece type)
would make it easier 
to see what incremental changes are available,
and unnecessary to filter illegal steps later,
especially as compared to doing coordinate arithmetic.
For another example, I think jumping would be 
more easily and reliably implemented 
in the piece type's step function.

\end{plSection}%{An executive}
%-----------------------------------------------------------------
\begin{plSection}{Rules of checkers}

How is
{\schemeFont 
(define-evolution-rule 'jump checkers 
(lambda (pmove) {\itshape body}))}
fundamentally different from 
{\schemeFont 
(define (jump pmove) {\itshape body})},
with the checkers game object 
holding a collection of such functions.
Especially since in reality one wouldn't want to generate all
possible moves, and would want the ability to compare
different strategies for generating the next move.

Similar comments hold for the aggregate rules.

\end{plSection}%{Rules of checkers}
%-----------------------------------------------------------------
\begin{plSection}{Critique}

Again, pretty self-congratulatory about things I disagree with.

They miss the critical issue of how hard it is to get
``rules'' like this right.

They only partially recognize the fact 
that what they are doing is just function composition,
and completely miss the key issue 
for valid moves is constraining which functions can be composed
with which, and which must be composed with which 
(eg for forcing jumps).

\end{plSection}%{Critique}
%-----------------------------------------------------------------
\end{plSection}%{2.4.2 Factoring out the domain}
%-----------------------------------------------------------------
\begin{plSection}{A sketch of something better}

As of 2021-05-08, I'm considering how much to do with this.
My complaints would be more convincing if I ``put up or shut up''.
But how much effort is it worth?
Does it illustrate something I would want in my domain:
computable science and mathematics?

\begin{description}
\item[Players]
\item[State of play]
\item[Pieces]
\item[Board (graph?) topology]
\item[Positions vs coordinates]
\item[Actions/Moves as functions]
\item[Legality constraints]
\end{description}


\end{plSection}%{A sketch of something better}
%-----------------------------------------------------------------
\end{plSection}%{2.4 Abstracting a domain}
%-----------------------------------------------------------------
\begin{plSection}{2.5 Summary}

This section repeats the mistakes from earlier in the chapter:
\begin{itemize}
  \item talks about ``lexical scoping'' rather than 
  first class functions
  \item ``combinators'' rather than first class compositions
  \item claim regular expression DSL is ``nicer'' in some 
  unspecified sense, and only suffers from verboseness
  \item views ``units of measurement'' as about wrappers,
  rather than the real problem: recognizing the difference
  between elements of a mathematical structure 
  (measurable quantity group) and differing coordinate systems
  for specifying those elements.
  \item confuses DSL with partitioning a problem,
  particularly the problem state, into simple components
  representing natural abstractions with as limited interfaces
  as possible, with the goal of being able to solve new, 
  but related problems by substituting variations on a few 
  components.
  If you do this right, you are creating new \emph{vocabulary},
  not a new \emph{language}.
  
  When people do create new \emph{languages},
  that's almost always a bad idea,
  and the problem would be better handled by choosing a better
  base language.
  The results (eg Unix little languages) are fragile systems built
  from islands of meaning separated 
  by gratuitously incompatible syntaxes,
  with endless bugs resulting from miscommunication.
\end{itemize}
\end{plSection}%{2.5 Summary}
%-----------------------------------------------------------------
\end{plSection}%{2 Domain Specific Languages}
%-----------------------------------------------------------------
\begin{plSection}{3 Variations on an arithmetic theme}
%-----------------------------------------------------------------
\begin{plSection}{Preview}

This chapter concerns ``generic procedures''
(aka generic functions aka multimethods {\ldots}),
something more important than anything in chapter 2.
It probably should have been the first content chapter,
so that ideas/code developed here could have been used 
in the rest of the book.

Another language/naming problem:
This section is not about arithmetic.
That's just part of the first example.
Could also look at the problem as another case of 
``burying the lead''.

Unfortunately the presentation is murky.
The value, if not the absolute necessity, of generic procedures
is easy to demonstrate with a wide variety of examples
that would be meaningful to typical SDEs,
but here is hidden in poorly thought-thru fragments of a symbolic
algebra/calculus system.

They repeat a fairly pervasive mistake: they treat mathematical
structures as though they were like types, in the sense that they
are static, global entities created at compile-time.
It's not hard to see that, in reality, software implementations
of mathematical structures must be dynamically created
objects, at most instances of types rather than types themselves.

Consider the minimal group-like algebraic structures 
called \emph{magmas}~\cite{wiki:Magma}.
Each of these is a set of elements and a group operation.
We get two different structures
if we take the set to be {\javaFont int},
and the operation to be {\javaFont +} versus {\javaFont *}

\NOTE For this to be correct we have to choose what happens 
when these operations overflow the range of {\javaFont int}.
Two obvious options:
\begin{itemize}
  \item default Java behavior is twos-complement wrap-around: 
  {\javaFont Integer.MAX\_VALUE + 1 -> Integer.MIN\_VALUE}.
I suspect there is a lot of Java code with unrecognized bugs due
to developers not considering this issue.
\item Overflow into some unbounded integer representation.

\NOTE {\javaFont java.math.BigInteger} doesn't in fact work here,
because it has a bounded range. 
There's nothing fundamental that would prevent overflowing
from {\javaFont java.math.BigInteger}
into some unbounded integer representation,
but in practice, it's unlikely to be worth the trouble,
since, at present, a calculation that would overflow
would either give an out-of-memory error first,
or take so long to get to the overflow that no one would still be 
alive to see the answer {\ldots}.
  
\end{itemize}

This is the necessary clear thought  in problem formulation
that's pervasively missing from the book,
and is the most critical requirement if you want
``to avoid programming yourself into a corner''.

\end{plSection}%{Preview}
%-----------------------------------------------------------------
\begin{plSection}{3.1 Combining arithmetics}

``Arithmetic''? Never clearly defined, and desperately needs 
careful thought if you want to get it right.

The closest they get is:
``Suppose we have a program that computes some useful numerical 
results.
It depends on the meanings of the arithmetic operators 
that are referenced in the program text.
These operators can be extended to work on things
other than the numbers that were expected by the program.
{\ldots}
A common pattern is a program that takes numerical weights and 
other arguments and makes a linear combination by adding up
the weighted arguments.''

``A common pattern'' should be `` a classic blunder''.
They go on to amplify their mistake by, again, confusing
tuples of numbers with vectors, and ignoring the fact that,
while the meaning of ``+'' for vectors for vectors is fairly
clear, it only is valid for vectors in the same \emph{space}:
same dimension is not enough. 
For an example of how badly this can go wrong, 
consider R~\cite{GentlemanEtal:2011:R},
where ``adding'' two tuples of different lengths is implemented
by recycling the elements of the shorter array.
(I believe this is due the R represented scalars by 1-element
arrays, and making the bad choice that the same operator
should be used to add vectors and add a scalar to each coordinate
of a vector, the second an operation that doesn't make sense
in vector spaces anyway.)

And what about ``*'' for vectors? 
Inner product? Outer/tensor product?
Or element-wise multiply of particular coordinates, 
which again doesn't make sense for vectors.

They do note that extending ``+'' and ``*'' to matrices
runs into problems, but: ``We will ignore this problem for now.''
They fail to recognize that the reason it doesn't work is
because matrix ``times'' vector is really function application,
and matrix ``times'' matrix is really function composition.

%-----------------------------------------------------------------
\begin{plSection}{MIT Scheme numbers}

The only way to get a handle on what they might have in mind
is to look at numbers and arithmetic operators 
in MIT Scheme~\cite[chapter 4]{Hanson:2020:MITSchemeRef}.

MIT Scheme numbers are classified in three dimensions: 
type, exactness, and implementation, which \emph{do not} have
a simple relationship.

\begin{description}
\item[Types:]
The numerical type tower is:
{\schemeFont 
integer? $\Rightarrow$ rational? $\Rightarrow$ real? 
$\Rightarrow$ complex? $\Rightarrow$ number?}.
``{\ldots} the types number, complex,
real, rational, and integer to refer 
to both mathematical numbers and Scheme numbers.'',

There are couple of issues here that are being ignored.

The meaning of ``real'' is complicated question,
both in mathematics and computations,
and deserves more attention.
See, for example, \citeAuthorYearTitle{Henle:2012:RealNumbers}.

Mathematicians rely on ambiguity that isn't possible 
in computation.
An example of this: 
Is $\Space{Q}$ a subset (actually a sub-field) of $\Space{R}$?
Or is it isomorphic to a sub-field of $\Space{R}$?
Mathematicians are free to switch between these two viewpoints
whenever convenient, or hold both at the same time.

\item[Exactness:]
Every MIT Scheme number is either {\schemeFont exact?} or 
{\schemeFont inexact?}.
Unfortunately, it pretty unclear from the documentation
what exactly this is supposed to mean.
It seems like the idea is to mark some data as involving
an approximation, but not include any info about what the
error bounds might be.
It doesn't take much thought to see how murky this idea is,
and I'm not sure it has much value, in any case.

``A number is exact if it was written as an exact constant 
or was derived from exact numbers using only exact operations.
A number is inexact if it was written as an inexact constant, 
if it was derived using inexact ingredients,
or if it was derived using inexact operations. 
Thus inexactness is a contagious property of a 
number.''~\cite[section 4.2]{Hanson:2020:MITSchemeRef}

However, this is contradicted shortly after:
``An operation may, however,
return an exact result if it can prove that the value of the 
result is unaffected by the inexactness of its arguments. 
For example, multiplication of any number by an exact zero
may produce an exact zero result, even if the other argument 
is inexact.''
They require only 
that alternate implementations return the same results
if the intermediate operations are all exact
(even though not the same intermediate operations).

The operation {\schemeFont inexact->exact} is described
as returning the closest exact number to it's input,
which doesn't make sense---a {\schemeFont flonum} has
an exact value as a rational or real.
I can't think of an example where an inexact number can only
be approximated by an exact one.

IEEE binary64 numbers are an exact representation of a subset of
$\Space{Q}$. 
Floating point arithmetic is ``exact'';
it just doesn't have the same rules as rational arithmetic.
Where you get into trouble is confusing  
{\schemeFont flo:+} with the missing {\schemeFont rational:+},
and never specifying exactly what the unqualified 
{\schemeFont +}, which is presumably meant to be
{\schemeFont complex:+}.

What's missing here is anything explicit about approximation,
or rounding (though there is some discussion of rounding modes
for {\schemeFont flonum} operations).

\item[Implementation]
``MIT/GNU Scheme implements the whole tower of numerical types. 
It has unlimited-precision exact
integers and exact rationals. 
Flonums are used to implement all inexact reals; 
on machines that support IEEE floating-point arithmetic 
these are double-precision floating-point 
numbers.''~\cite[chapter 4, footnote 1]{Hanson:2020:MITSchemeRef}

The reference manual discusses only two implementations,
and those are not fully first class:
\begin{description}
\item[fixnum:] a subset of integers
represented by a twos-complement bit sequence 
of unspecified length.
There is no API to get at the number of bits---the manual
has a code fragment illustrating a search to find the max/min
fixnum.
\item[flonum] ``MIT/GNU Scheme follows the IEEE 754-2008 
floating-point standard, using binary64
arithmetic for 
flonums.''~\cite[section 4.7.2]{Hanson:2020:MITSchemeRef}
There doesn't appear to be any straightforward way to get
at the bits, like, for example, 
{\javaFont Double.doubleToLongBits},
though there functions to get at the exponent, word-size, etc.,
of the floating point implementation. 
(Given that they assert IEEE binary64, 
these functions are not strictly necessary.)
\end{description}
All other implementations are hidden, 
which is a fundamental problem if the idea is to extend arithmetic
with these functions as a base.

In addition, the lack of simple access to primitives
makes doing an alternative number library infeasible
in MIT Scheme, though perhaps the FFI might allow something
in C.~\cite{Birkholz:2018:MITSchemeSOS}
\end{description}

\end{plSection}%{MIT Scheme numbers}
%-----------------------------------------------------------------
\begin{plSection}{3.1.1 A simple ODE integrator}

Looking at MIT Scheme numbers and arithmetic didn't help
illuminate what they mean by ``arithmetic''.
Perhaps the first example will?

In this case, the topic is simulating the behavior 
of a physical system specified by an ordinary differential
equation (ODE), that is, a univariate differential equation.
There is no context for the problem, not even references.

A couple possibly relevant ones are: 
\citeAuthorTitle{wiki:VerletIntegration}
and
\citeAuthorYearTitle[%
section 17.4 Second-order conservative equations]%
{PressEtal:2007:NumericalRecipes}.

I've spent a couple days rereading this section,
and Numerical Recipes, and have decided to skip this example.

Like chapter 2, they jump into it
without the motivation, background, and bigger picture 
needed for reasoned design choices.
They only use it briefly, a couple times,
to illustrate arithmetic over symbolic expressions.
This makes no sense, since St\"{o}mer's integrator
is a method \emph{numerically} approximating the solution
to a differential equation, so what's the point?
They suggest that the expanded expression that results might
help with debugging, but that's dubious, even with 
``a magic symbolic-expression 
simplifier''.~\cite[p 96]{HansonSussman:2021:SDFF}

Another symptom of the lack of realistic context for the example: 
Using St\"{o}mer's integrator, in this case,
to estimate $x(t+h)$, requires 
3 initial values: $x{t}, x(t-h), x(t-2h)$.
They don't give any indication of how one might determine those
in general.
Instead they take the example of $D^{2} x(t) \,=\, - x(t)$,
where they happen to know the solution space. 
They them use a particular solution, $\sin (t)$,
to initialize the integrator.
Of course, if you know the solution space, the integrator is
at best irrelevant.

I suppose it could be useful to take cases where the solution 
is known as a way of testing the integrator,
but even that is non-trivial.
For approximation problems, it is usually difficult to know how
close to the ``true'' answer the result should be,
so it is hard to write a test that doesn't give many false
positives/negatives.
Even for comparing the accuracy of alternate integrators,
you are left with the issue of whether such simple equations
are representative of real problems.

\NOTE Missed opportunity: comparison with the C++ implementation
in  Numerical Recipes.

% One aspect that seems relevant to the question of ``arithmetic''
% is the fact that they claim they are solving the differential
% equation
% \begin{equation}
% D^{2} x \left( t \right) \, = \, F\left(t,x\left(t\right)\right)
% \end{equation}
% when they are in fact modelling the behavior 
% with a discrete difference equation.
% 
% Any issues of alternative approximations and accuracy trade-offs,
% or whether difference equations might be the right setting from 
% the start,
% are considered to be in the domain of ``numerical analysis
% and are not of interest here.''
% 
% They fail to mention that what they are doing is
% approximating the solution to an \emph{initial value problem}.
% This is important because the designer needs to know if
% we have decided that we are only interested in initial value
% problems (and why). Or would solving other boundary conditions
% eventually be necessary?
% 
% \TODO
% Might be better to do this after discussing number systems.
% What do we need for differential equations?
% Rationals? Computable reals?
% 
% \NOTE 
% Let's see if we can make more sense of the ``stepper''
% they write without any justification:
% 
% \begin{equation}
% \Delta_{h} x(t) \; \defeq \;
% \frac
% {x\left(t+\frac{h}{2}\right) \,-\, x\left(t-\frac{h}{2}\right)}
% {h} 
% \end{equation}
% so
% \begin{align}
% \Delta_{h}^{2} x(t) & \; = \; 
% \frac{
% \Delta_{h}x\left(t+\frac{h}{2}\right)
%  \,-\, 
% \Delta_{h}x\left(t-\frac{h}{2}\right)}
% {h} \\ 
% & \; = \; 
% \frac
% {x\left(t+h\right) \,-\, x\left(t\right)
%  - \left[x\left(t\right) \,-\, x\left(t-h\right)\right]}
% {h^2} \nonumber \\ 
% & \; = \; 
% \frac{x\left(t+h\right) \,-\, 2 x\left(t\right)
% + x\left(t-h\right)}
% {h^2} \nonumber \\
% & \; = \; 
% F\left(t,x\left(t\right)\right) \nonumber 
% \end{align}
% which lets us re-write SDFF equation 3.2 in a slightly more
% informative way:
% \begin{equation}
% \Delta_{h}^{2} x(t)  \; = \; 
% \sum_{j=0}^{k}
% A(j)\, F \left( t-jh, x(t-jh) \right)
% \end{equation}
% where ``$A$ is the array of magic coefficients.''

\end{plSection}%{3.1.1 A simple ODE integrator}
%-----------------------------------------------------------------
\begin{plSection}{3.1.2 Modulating arithmetic operators}

``Modulating''?

I think ``arithmetic'' is a good example for generic functions,
but not because it is simple and familiar
(which is what I guess Hanson and Sussman are assuming).
Rather just the opposite---there are many subtleties
that require thought to recognize,
and careful consideration in making design tradeoffs
in performance vs generality, allowing necessary user control,
etc.,
and accepting that there is no single right answer.

I wouldn't use ``arithmetic'' as a first example for
generic functions, but would introduce it later.

See \cref{sec:NumberSystems} 
for a more thorough discussion 
of what "arithmetic" should or might mean,
an example of the kind of preparation needed 
to avoid ``programming ourselves into a corner''.

%-----------------------------------------------------------------
\begin{plSection}{Problems with redefining operators}

Nothing about this is special to generic functions.
Same issue with redefining functions in general.
Real issue is mutating shared global state,
which is of course dangerous, but sometimes necessary.
Dealing with mutable global state properly requires
clearly identifying it, minimizing it,
control over who can mutate it,
when it can be mutated,
notification on mutation, {\ldots},
all of which they fail to do.

\end{plSection}%{Problems with redefining operators}
%-----------------------------------------------------------------
\end{plSection}%{3.1.2 Modulating arithmetic operators}
%-----------------------------------------------------------------
\begin{plSection}{3.1.3 Combining arithmetics}
%-----------------------------------------------------------------
\begin{plSection}{An improved arithmetic abstraction}
\end{plSection}%{An improved arithmetic abstraction}
%-----------------------------------------------------------------
\begin{plSection}{A combinator for arithmetics}

Calling this a ``combinator'' doesn't help anything.

\end{plSection}%{A combinator for arithmetics}
%-----------------------------------------------------------------
\begin{plSection}{3.1.4 Arithmetic on functions}

Very sketchy without first class (co)domains.

\end{plSection}%{3.1.4 Arithmetic on functions}
%-----------------------------------------------------------------
\begin{plSection}{3.1.5 Problem with combinators}

Again, calling this a ``combinator'' just hides the real issues.

\end{plSection}%{3.1.5 Problem with combinators}
%-----------------------------------------------------------------
\end{plSection}%{3.1 Combining arithmetics}
%-----------------------------------------------------------------
\begin{plSection}{3.2 Extensible generic procedures}
%-----------------------------------------------------------------
\begin{plSection}{3.2.1 Generic arithmetic}
%-----------------------------------------------------------------
\begin{plSection}{Fun with generic arithmetics}
\end{plSection}%{Fun with generic arithmetics}
%-----------------------------------------------------------------
\end{plSection}%{3.2.1 Generic arithmetic}
%-----------------------------------------------------------------
\begin{plSection}{3.2.2 Construction depends on order}
\end{plSection}%{3.2.2 Construction depends on order}
%-----------------------------------------------------------------
\begin{plSection}{3.2.3 Implementing generic procedures}
%-----------------------------------------------------------------
\begin{plSection}{Making constructors for generic procedures}
\end{plSection}%{Making constructors for generic procedures}
%-----------------------------------------------------------------
\begin{plSection}{Adding handlers to generic procedures}
\end{plSection}%{Adding handlers to generic procedures}
%-----------------------------------------------------------------
\begin{plSection}{The power of extensible generics}
\end{plSection}%{The power of extensible generics}
%-----------------------------------------------------------------
\end{plSection}%{3.2.3 Implementing generic procedures}
%-----------------------------------------------------------------
\end{plSection}%{3.2 Extensible generic procedures}
%-----------------------------------------------------------------
\begin{plSection}{3.3 Example: automatic differentiation}
%-----------------------------------------------------------------
\begin{plSection}{3.3.1 How automatic differentiation works}
%-----------------------------------------------------------------
\begin{plSection}{Extending the primitives}
\end{plSection}%{Extending the primitives}
%-----------------------------------------------------------------
\begin{plSection}{Extracting the derivative's value}
\end{plSection}%{Extracting the derivative's value}
%-----------------------------------------------------------------
\end{plSection}%{3.3.1 How automatic differentiation works}
%-----------------------------------------------------------------
\begin{plSection}{3.3.2 Derivatives of n-ary functions}
\end{plSection}%{3.3.2 Derivatives of n-ary functions}
%-----------------------------------------------------------------
\begin{plSection}{3.3.3 Some technical details}

\citeAuthorYearTitle{SiskindPearlmutter:2005:PerturbationConfusion}

\citeAuthorYearTitle{ManzyukEtAl:2019:PerturbationConfusion}

%-----------------------------------------------------------------
\begin{plSection}{Differential algebra}
\end{plSection}%{Differential algebra}
%-----------------------------------------------------------------
\begin{plSection}{Finite and infinitesimal parts}
\end{plSection}%{Finite and infinitesimal parts}
%-----------------------------------------------------------------
\begin{plSection}{How extracting really works}
\end{plSection}%{How extracting really works}
%-----------------------------------------------------------------
\begin{plSection}{Higher order functions}
\end{plSection}%{Higher order functions}
%-----------------------------------------------------------------
\end{plSection}%{3.3.3 Some technical details}
%-----------------------------------------------------------------
\end{plSection}%{3.3 Example: automatic differentiation}
%-----------------------------------------------------------------
\begin{plSection}{3.4 Efficient generic procedures}

A ``master class'' ought to assume familiarity with
indexing data structures, just describe some relevant 
libraries, rather than building from scratch.

Missed opportunity: careful benchmarking as-you-go
is an under appreciated aspect of designing for flexibility.
Performance matters. 
Missing a small design change with big
implications for time and space can greatly limit later options.

%-----------------------------------------------------------------
\begin{plSection}{3.4.1 Tries}
\end{plSection}%{3.4.1 Tries}
%-----------------------------------------------------------------
\begin{plSection}{3.4.2 Caching}
\end{plSection}%{3.4.2 Caching}
%-----------------------------------------------------------------
\end{plSection}%{3.4 Efficient generic procedures}
%-----------------------------------------------------------------
\begin{plSection}{3.5 Efficient used-defined types}
%-----------------------------------------------------------------
\begin{plSection}{3.5.1 Predicates as types}

Confusion about predicates, sets, types,
dynamic vs static,
local vs global.

\end{plSection}%{3.5.1 Predicates as types}
%-----------------------------------------------------------------
\begin{plSection}{3.5.2 Relationships between predicates}

\end{plSection}%{3.5.2 Relationships between predicates}
%-----------------------------------------------------------------
\begin{plSection}{3.5.3 Predicates are dispatch keys}
 
Bad English, should be ``Predicates \emph{as} dispatch keys''.

Bad concepts, should ``\emph{Sets} as dispatch keys''.
 
\end{plSection}%{3.5.3 Predicates are dispatch keys}
%-----------------------------------------------------------------
\begin{plSection}{3.5.4 Example: an adventure game}
 
Bad example, would have more useful in the checkers/chess
section as something to consider when formulating the 
games problem.

Use of generic procedures is trivial compared
to math examples, and example needs too much supporting code
for no significant lessons about generic procedures.
 
%-----------------------------------------------------------------
\begin{plSection}{Implementing properties}
Unnecessary in a ``master class''.
\end{plSection}%{Implementing properties}
%-----------------------------------------------------------------
\end{plSection}%{3.5.4 Example: an adventure game}
%-----------------------------------------------------------------
\end{plSection}%{3.5 Efficient used-defined types}
%-----------------------------------------------------------------
\begin{plSection}{3.6 Summary}
\end{plSection}%{3.6 Summary}
%-----------------------------------------------------------------
\end{plSection}%{3 Variations on an arithmetic theme}
%-----------------------------------------------------------------
\end{plSection}%{3 Variations on an arithmetic theme}
%-----------------------------------------------------------------
\BeginAppendices
%-----------------------------------------------------------------
\begin{plSection}{Number systems and arithmetic}
\label{sec:NumberSystems}

I think when most software developers see ``arithmetic''
they remember elementary school
and think of addition, subtraction, multiplication, and division.
The ``numbers'' passed to these operations are 
understood to be the (classical) reals, 
though probably only partially understood.

This adds up to what a mathematician would call an ordered 
\emph{field}~\cite{wiki:FieldMathematics}.
Skipping the ``ordered'' part for now,
a field $\Space{F}$ consists of
$[\Set{F},\Add{\Space{F}},\Mul{\Space{F}},
\Sub{\Space{F}},\Div{\Space{F}},\Zero{\Space{F}}, \One{\Space{F}}]$:
\begin{description}
\item[Elements:] a set $\Set{F}$, the ``numbers''.
\item[Addition:] a binary operation (two argument function)
$\Add{\Space{F}} : \Set{F} \times \Set{F} \rightarrow \Set{F}$
\item[Multiplication:] a binary operation 
$\Mul{\Space{F}} : \Set{F} \times \Set{F} \rightarrow \Set{F}$
\item[Additive inverse:] a unary operation (one argument function)
$\Sub{\Space{F}} :\Set{F} \rightarrow \Set{F}$.
\item[Multiplicative inverse:] 
a unary operation (one argument function)
on everything but the additive identity:
$\Div{\Space{F}} :  \Set{F} \setminus \left\{\Zero{\Space{F}}\right\} 
\rightarrow \Set{F}$.
\item[Additive identity:] a nullary operation 
(zero argument function)
$\Zero{\Space{F}} : \emptyset \rightarrow \Set{F}$.
Mathematicians typically ignore the difference between a zero
argument function and the value of that function,
acting as if $\Zero{\Space{F}}() = \Zero{\Space{F}}\, \in \, \Set{F}$.
\item[Multiplicative identity:] a nullary operation 
$\One{\Space{F}} : \emptyset \rightarrow \Set{F}$.
\end{description}
Note that I am being pedantic about a number of things
that mathematicians usually leave ambiguous,
such as the difference between the field $\Space{F}$ 
and the set of elements $\Set{F}$,
and labeling the operations
with the field for which they are defined.
Another non-standard aspect of my definition is taking
the identities to be zero-argument functions rather than
elements of the set.
The reason for these peculiarities is to better match the needs 
of implementation, which should, I hope, 
become clear later.

The operations satisfy the following laws, for all 
$a, b, c \,\in \, \Set{F}$
\begin{description}

\item[Additive identity:] 
$\Zero{\Space{F}} \,\Add{\Space{F}}\, a \,=\, a$.

\item[Multiplicative identity:] 
$\One{\Space{F}} \,\Mul{\Space{F}}\, a \,=\, a$.

\item[Additive inverse:] 
$\Sub{\Space{F}}(a) \;\Add{\Space{F}}\; a = \Zero{\Space{F}}$.
Binary subtraction is defined as the composition 
of addition and inverse:
$a \,\Sub{\Space{F}}\, b = 
a \,\Add{\Space{F}}\, \left(\Sub{\Space{F}}(b)\right) $

\item[Multiplicative inverse:] 
$\Div{\Space{F}}(a) \;\Mul{\Space{F}}\; a = \One{\Space{F}}$ for all $a \neq \Zero{\Space{F}}$.
Binary division is defined as with binary addition,
with the wrinkle that the second argument (the divisor)
cannot be the additive identity.

\item[Associative addition:] 
$\left( a \Add{\Space{F}} b \right)  \Add{\Space{F}} c 
\,=\, 
a  \Add{\Space{F}} \left( b  \Add{\Space{F}} c \right)$

\item[Associative multiplication:] 
$\left( a  \Mul{\Space{F}} b \right) \Mul{\Space{F}} c 
\,=\, 
a \Mul{\Space{F}} \left( b \Mul{\Space{F}} c \right)$

\item[Commutative addition:] 
$a \Add{\Space{F}} b \,=\, b \Add{\Space{F}} a$

\item[Commutative multiplication:] 
$a \Mul{\Space{F}} b \,=\, b \Mul{\Space{F}} a$

\item[Distributive multiplication over addition:] 
$a \, \Mul{\Space{F}} \, \left( b \Add{\Space{F}} c \right)
\,=\, 
\left(a \Mul{\Space{F}} b \right) 
\, \Add{\Space{F}} \, 
\left(a \Mul{\Space{F}} c \right)$

\end{description}

The problem with this viewpoint is that computer arithmetic
doesn't work this way
and, more critically, computer arithmetic \emph{can't} work
this way.
The (classical) reals, $\Re$, are \emph{uncountable};
computer arithmetic can at most handle countably infinite sets
(in the sense that the representable values are 
bounded by available memory (and time),
rather than having an intrinsically limited range)

This suggests the following question:
Is there a countable, implementable number system,
probably a subset of $\Re$,
which is sufficient for, in my case,
``scientific computing''?

Perhaps surprisingly, the answer to this seems to be yes.
See: 

\citeAuthorYearTitle{Feferman:1989:IsCantorNecessary}

\citeAuthorYearTitle{Feferman:1992:ALittleBit}

Reprinted, updated in:

\citeAuthorYearTitle[ch.~2 ``Is Cantor necessary?'']{
Feferman:1998:LightOfLogic}

\citeAuthorYearTitle[ch.~12 ``Is Cantor necessary? (Conclusion)'']
{Feferman:1998:LightOfLogic}

\citeAuthorYearTitle[ch~14 ``Why a little bit goes a long way:
logical foundations of scientifically applicable mathematics'']{
Feferman:1998:LightOfLogic}

Useful references, intended for undergraduates, include:
 
\citeAuthorYearTitle{Feferman:1989:NumberSystems}

\citeAuthorYearTitle{Henle:2012:RealNumbers}

\citeAuthorYearTitle{Bridger:2019}

In the next few sections I'm going to work thru a series
of number systems, both mathematical and computational.
My approach is to motivate a new structure
by posing a problem that can't be solved in the structures
introduced up to that point.
This leads to an implementation/representation
in terms of previous (structures),
which can then be abstracted into a set of axioms.
The goal is to end up with something that is both
implementable and covers ``scientifically applicable mathematics''.

%-----------------------------------------------------------------
\begin{plSection}{Nonstandard foundations}

\begin{plQuote}
{\citeAuthorYearTitle{Thurston:1994:Proof}}{}%
{Mathematics as we practice it is much more formally 
complete and precise than
other sciences, but it is much less formally complete and precise 
for its content than computer programs.
}
\end{plQuote}

The mathematics I will be using is unusual in two ways:

\begin{itemize}
  
\item Universal quantification only.

This is related to, but not exactly the same as,
universal algebra~\cite{wiki:UniversalAlgebra}.
The idea here is that the laws (axioms) 
in the definition of a mathematical structure
will be expressed using
universal (``for all'') quantifiers, 
not existential (``there exists") ones.

For example, the classical definition of 
\emph{monoid}~\cite{wiki:Monoid} $\Space{M} = [\Set{M}, \odot]$
would be:
\begin{description}
  \item[Elements:] The set $\Set{M}$ is given.
  \item[Binary operation:] The function
  $\odot : \Set{M} \times \Set{M} \rightarrow \Set{M}$ is given.
  Implied here is 
  $m_0 \odot m_1 = \odot(m_0,m_1) \in \Set{M};
  \text{ for all } m_0, m_1 \in \Set{M}$.
  \item[Associativity:] For all $m_0, m_1, m_2 \in \Set{M}$,
  $(m_0 \odot m_1) \odot m_2 \;=\; m_0 \odot (m_1 \odot m_2).$
  \item[Identity:] There exists $e \in \Set{M}$ such that
  $e \odot m \,=\, m \odot e \,=\, m$,
  for all $m \in \Set{M}$. 
\end{description}
The universal version treats the identity as given
(as a nullary/zero-argument/constant) function.
\begin{description}
  \item[Identity:] Given a zero-argument function 
  $e() : \emptyset \rightarrow \Set{M}$ such that
  $e() \odot m \,=\, m \odot e() \,=\, m$,
  for all $m \in \Set{M}$. 
\end{description}
The difference may seem subtle, but the universal approach
helps with test generation.
Proving an implementation satisfies all the laws is difficult 
in either case, but is it easier to validate
universal laws with generative tests.

\item Computable/constructive/intuitionist mathematics

(To be honest, this is not fully worked out.)

The basic idea here is to start from an
idealized, Turing machine equivalent,
virtual machine for ``procedures'', and define 
``computable mathematics'' in terms of what can be computed 
in a finite number of steps.
I believe this restricts us to at most countable sets,
and eliminates some of the paradoxes that led to the crisis
in the foundations of mathematics circa 1900.
More important in a practical sense, a mathematics that is
defined by finitary procedures translates more-or-less
directly into physical computation---the only difference is 
that real machines are bounded in space and time.

There is at least some reason to think 
that this kind of foundation covers all ``scientifically
applicable mathematics''.
See:

This approach to mathematics eliminates much of the ugliness
of classical analysis---no axiom of choice, no unmeasurable
sets, etc.

However, more or less in exchange, it adds other complexities,
related to the fact that one can't in general tell whether a given
procedure with a given input, will ever halt.
This modifies our idea of ``true'' and ``false''.
A statement is ``true'' if there is a finite procedure that
proves it true, and the same for false,
but we have a third state: not halting. 
And we can't be sure whether a running procedure won't halt,
or is just very slow.

\end{itemize}

\end{plSection}%{Nonstandard foundations}
%-----------------------------------------------------------------
\begin{plSection}{Natural numbers}

I'm going to take the \emph{natural numbers},
$\Naturals =
 [ \Set{N}=\left\{ 0, 1, 2, {\cdots}\right\}, \Add{\Naturals}]$,
and the usual notion of addition on them $\Add{\Naturals}$,
as given.

(You can start from less than this if you like.
What's essential is a starting element, eg, $0$,
and a {\pseudocodeFont next}
or {\pseudocodeFont increment} operation. 
See, for example, 
\citeAuthorYearTitle[section 3.1]{Feferman:1989:NumberSystems}.
However, this kind of axiomatic minimalism 
isn't my primary interest.
What I want to do instead is to motivate the mathematical
structures I describe by posing at least somewhat realistic
problems that they can be used to solve.)

In the case of $\Naturals$, I find it a bit difficult 
come up with something convincing without
jumping ahead to the rationals.
So, if this seems a bit too trivial,
 bear with me for the moment.

The natural numbers answer two kinds of questions:
\begin{itemize}
\item How many? (cardinality)
\item What's next? or Which is bigger? (ordinality)
\end{itemize}

Addition, $\Add{\Naturals}$, answers questions like:
If I have $m$ loaves of bread,
and my friend has $n$, how many do we have all together?

The ordering tells us: Who has more bread?

\TODO: This isn't very satisfying.
Look into origins of math, Egypt, Mesopotamia, etc, for better
examples?

$\Naturals$ is a \emph{commutative monoid},
which is a monoid where the operation is symmetric:
\begin{description}
\item[Elements:] $\Set{N}$
\item[Operation:] $\Add{\Naturals}$
\item[Identity:] $\Ide{\Add{\Naturals}}() \rightarrow \Zero{\Naturals}$
\item[Associative:] 
$(n_0 \Add{\Naturals} n_1) \Add{\Naturals} n_2 \;=\;
n_0 \Add{\Naturals} (n_1 \Add{\Naturals} n_2)$, 
for all $n_0, n_1, n_2 \in \Set{N}$
\item[Commutative:] $n_0 \Add{\Naturals} n_1 \;=\;
n_1 \Add{\Naturals} n_0$,
for all $n_0, n_1 \in \Set{N}$
\end{description}

\TODO: missing $<$, $\leq$ as operator.
Should this be \emph{ordered commutative monoid},
or deal with ordering separately?

Multiplication, $\Mul{\Naturals}$, can be derived from addition in the
obvious way.
This gives us another commutative monoid:
$[\Set{N}, \Mul{\Naturals}, \Ide{\Mul{\Naturals}}]$.

Monoids are examples of what sometimes are referred to 
as group-like structures~\cite{wiki:GroupLike},
but might be better called ``one set, one (binary) operation''
structures.

We can also consider $\Naturals$ as a ring-like
(one set, two operation)
structure, a \emph{commutative semi-ring}~\cite{wiki:Semiring}, 
using both $\Add{\Naturals}$ and $\Mul{\Naturals}$,
which then obeys the commutative monoid laws for
$\Add{\Naturals}$ and $\Mul{\Naturals}$, and two more laws in addition:
\begin{description}
\item[Distributive:] 
$(n_0 \Add{\Naturals} n_1) \; \Mul{\Naturals}\; n_2 \;=\;
(n_0 \Mul{\Naturals} n_2) \; \Add{\Naturals} \; (n_1 \Mul{\Naturals} n_2)$, \\
for all $n_0, n_1, n_2 \in \Set{N}$.
\item[Zero:] $\Ide{\Add{\Naturals}} \Mul{\Naturals} n 
\; = \; \Ide{\Add{\Naturals}}
\; = \; 0$ for all $n \in \Set{N}$
\end{description}

This is not particularly important, but it serves as 
a very simple example
of how multiple mathematical structures can be defined
over a single set.
This does have significant implications for implementations.
It's tempting, and a common mistake, 
to represent mathematical structures by types or classes.
We might have, say, a Java class that provides an
unbounded implementation of the natural numbers,
with methods for $\Add{\Naturals}$, $\Mul{\Naturals}$, 
$\Ide{\Add{\Naturals}}$,
and $\Ide{\Mul{\Naturals}}$.
But that locks us into the semi-ring view,
disallowing the monoid views.

\TODO In this particular case, that doesn't seem so bad,
but we will see more convincing examples later.

%-----------------------------------------------------------------
\begin{plSection}{Modular arithmetic}

Another example of why it would be a mistake to implement
$\Naturals$ as, in Java terms,
a class implementing some {\javaFont CommutativeMonoid} interface,
is the fact that there are many other important structures
to be defined on subsets of $\Set{N}$, which require
different definitions for $\Add{\Naturals}$ and $\Mul{\Naturals}$.

The too simple motivating question here might be:
It is the $20$th hour of the day. What hour will it be $5$ hours
from now?

To deal with questions like this, we need to modify 
$\Add{\Naturals}$,
give up ordering, and get, as a result, two new \emph{families} of 
commutative monoids (or a family of semi-rings, 
if we prefer to think of of it that way).

Consider the \emph{intervals} in $\Set{N}$:
For $n_{0}\,\leq\, n_{1}$, 
$\left[ n_{0}, n_{1} \right] \; \defeq \;
\left\{ n_{0}, n_{0} + 1, \cdots , n_{1} \right\}$.

(\NOTE To avoid off-by-one errors, 
I prefer to define all integer intervals as half-open.
But that is a bit difficult without subtraction.

\NOTE Modular arithmetic is more often defined for the special 
case of $n_0=0$. My reasons for this slightly more general 
version should become apparent soon.)

Then we can define a new addition operator 
$\Add{ \Naturals[n_0,n_1] }$:
\begin{plAlgorithm}
{Modular addition}
{modularAddition}
\begin{lstlisting}[language=pseudocode]
(defn $\Add{\Naturals[n_0,n_1]}$ [a b]
  (assert ($\Leq{\Naturals}$ 0 $n_0$ a $n_1$))
  (assert ($\Leq{\Naturals}$ 0 $n_0$ b $n_1$))
  (let [step (fn [sum count]
                    (if ($\Eq{\Naturals}$ count b)
                      sum
                      (if ($\Eq{\Naturals}$ sum $n_1$)
                        (step $n_0$ ($\Add{\Naturals}$ 1 count))
                        (step ($\Add{\Naturals}$ 1 sum) ($\Add{\Naturals}$ 1 count)))))]
    (step a 0)))
\end{lstlisting}
\end{plAlgorithm}
(\NOTE This is not intended to be a practical algorithm,
more a specification using only $\Add{\Naturals}$.)

\TODO Change $\Add{\Naturals}$ to something like
$\mathsf{\Naturals{.}+}$, 
$\mathsf{\Naturals{:}+}$, 
$\Naturals\mathsf{{:}+}$, 
$\Naturals\mathsf{{\colon}+}$, 
$\Naturals\mathsf{/+}$, 
$\mathsf{(+ \, \Naturals)}$, 
$\mathsf{+(\Naturals)}$, 
to look more like a package prefix or field value?

We can then define the commutative semi-group 
(a monoid without identity~\cite{wiki:Semigroup})
$
\Naturals[n_0,n_1] =
\left[ 
\left[ n_{0}, n_{1} \right], 
\Add{\Naturals[n_0,n_1]}
\right]
$.
(If $n_0=0$, then it's a commutative monoid.)

(A perhaps cleaner, or ``more pure'' version of this
would distinguish the elements of $\Naturals[n_0,n_1]$
from the elements of $\Naturals$,
and define $\Add{\Naturals[n_0,n_1]}$ via an isomorphism.
I'm using the ambiguity typical of mathematics to ignore that
distinction.)

\end{plSection}%{Modular arithmetic}
%-----------------------------------------------------------------
\begin{plSection}{Implementation}

Implementing $\Naturals$ is straightforward 
(as long as we ignore time and space constraints).
All we need is an implementation of unbounded bit sequences:

Any natural number $n$ can be written as
\begin{equation}
n \; = \; \sum_{k=0} \beta_{k}(n) * 2^{k}
\end{equation}
where $\beta_{k}(n)$ is $0$ or $1$,
and exponentiation over natural numbers is defined from
$\Mul{\Naturals}$ in the obvious way.
I'll note without proof that for any $n$ there is a 
$k_{\text{max}}(n)$ such that $\beta_{k}(n) \equiv 0$ for all
$k \geq k_{\text{max}(n)}$.
In other words, all such sequences are finite. 

Note also that we can define addition on bit sequences in the
obvious way, to match addition of the corresponding numbers.

The set of finite bit sequences 
$\Space{B} = \left\{[\beta_{0}, \beta_{1}, \cdots, 
\beta_{k_{\text{max}}]} \right\}$
 is a 
\emph{representation} of $\Naturals$
in the sense that we can define an invertible function,
$\beta : \Naturals \rightarrow \Space{B}$,
(an \emph{isomorphism}) between $\Naturals$ and 
finite bit sequences
that preserves addition (and ordering and identity):
\begin{equation}
\beta \left( a \,\Add{\Naturals} \, b\right)
\;=\;
\beta ( a ) \, \Add{\Space{B}} \, \beta ( b )
\end{equation}
(again without proof).

A proof-of-concept implementation in Java,
supporting just the commutative monoid over addition, 
can be found at
\href{https://github.com/palisades-lakes/sicpplus/blob/main/src/main/java/sicpplus/java/numbers/UnboundedNatural.java}
{\javaFont UnboundedNatural.java}.
This implements $\Naturals$ with an unbounded sequence
of {\javaFont int} 32 bit words, treated as unsigned.
Adding two such sequences of $1073741809$ words
takes more than $20$ minutes and 
runs out of memory in a 56g JVM.

A more practical implementation,
that offers better performance for most purposes,
can be found at: 
\href{https://github.com/palisades-lakes/sicpplus/blob/main/src/main/java/sicpplus/java/numbers/BoundedNatural.java}
{\javaFont BoundedNatural.java}.
However, 
like \href{https://github.com/openjdk/jdk/blob/master/src/java.base/share/classes/java/math/BigInteger.java}
{\javaFont java.math.BigInteger},
its range is bounded, so it only supports a subset of $\Naturals$.

{\javaFont BoundedNatural} and {\javaFont BigInteger}
both use {\javaFont int[]} arrays to hold their bits.
The maximum size of an array in Java is limited
by the fact that the JVM 
requires array sizes and indexes to be specified 
with {\javaFont int} values.
(See \citeAuthorYearTitle[section 6.5]{LindholmEtAl:2021:JVMS16}.)
This limits us to at most {\javaFont Integer.MAX\_VALUE} elements.
However, the actual maximum array length is strictly less than that,
by an amount that appears to be implementation dependent.
In my particular setup, attempting to create an 
{\javaFont int[Integer.MAX\_VALUE-1]} results in:
``{\javaFont java.lang.OutOfMemoryError: 
Requested array size exceeds VM limit}'',
even though there is plenty of memory available to hold 
an array of that size.
(I haven't been able to find any clear discussion of this in
\citeAuthorYearTitle{GoslingEtAl:2021:JLS16}.)

There is an even smaller limit on the size of numbers that can be
represented by {\javaFont BoundedNatural} and 
{\javaFont java.math.BigInteger},
due to the fact that both allow access to arbitrary bits
indexed by {\javaFont int}.
This means the maximum length bit sequence is 
{\javaFont Integer.MAX\_VALUE},
and the maximum number of words is then
{\javaFont Integer.MAX\_VALUE>>5}.

(We could increase this a bit, since the bit indexes need to fit
in an {\javaFont int}, but the number of bits could be $1$ more
than the maximum index, or {\javaFont Integer.MAX\_VALUE + 1L}.
This would however, complicate the code, requiring a fair amount
of mixing of {\javaFont int} and {\javaFont long})

{\javaFont BoundedNatural}, like {\javaFont java.math.BigInteger},
deals with operations that would overflow the supported range
by throwing an exception, 
so it doesn't implement a monoid or semi-ring 
or anything like that.

\TODO Consider whether our foundational definition of a procedure
should include error exit as a possible outcome, eg,
$ f : \Set{X} \rightarrow \Set{Y}$ may
return $y\in \Set{Y}$, throw an exception,
or never halt.

An alternative would be to overflow into an unbounded 
representation, which would be more work that I want to do 
at present.
In practice, time and space constraints limit how much useful
computation can be done close to the overflow limit.

\TODO Use JMH to measure cost of arithmetic with
{\javaFont UnboundedNatural},
{\javaFont BoundedNatural},
and primitives.

\TODO Should this discussion be moved to the integer section?
So that signed numbers make more sense? On the other hand, 
it is a simpler to just stick to one operation:
the $\Add{\Naturals}$ monoid.


Most C-family languages offer primitive fixed length
unsigned integers, with $8$, $16$, $32$, $64$, and 
sometimes $128$ bits.
Arithmetic on these numbers is implemented with one or a few
instructions, XXXX times faster than possible with something
like {\javaFont UnboundedNatural} or
{\javaFont BoundedNatural}.
What is implemented is actually modular arithmetic,
so each of these primitive types corresponds to a valid
semi-ring.
aHowever, I think this is poorly understood by many developers.
I suspect many lurking bugs due to unchecked overflows.

Java, on the other hand, doesn't support unsigned numbers.
The reasoning behind that is a bit obscure,
and the brief explanations in various places on the web
are unconvincing:

\begin{plQuote}
{\citeAuthorYearTitle{RitchieStroustrupGosling:2000:CFamily}}{}%
{%
Q: Programmers often talk about the advantages and disadvantages 
of programming in a "simple language."  
What does that phrase mean to you, and is [C/C++/Java] 
a simple language in your view? 

\ldots

Gosling: For me as a language designer, 
which I don't really count myself as these days, 
what "simple" really ended up meaning was could I expect 
J. Random Developer to hold the spec in his head. 
That definition says that, for instance, Java isn't---and 
in fact a lot of these languages end up 
with a lot of corner cases, things that nobody really understands. 
\emph{Quiz any C developer about unsigned, 
and pretty soon you discover 
that almost no C developers actually understand what goes on 
with unsigned, what unsigned arithmetic is.}[emphasis added] 
Things like that made C complex. 
The language part of Java is, I think, pretty simple. 
The libraries you have to look up.}

\ldots
\end{plQuote}

I don't think this is correct.
My guess is that few developers are confused about
what unsigned values are.
Some may not get modular arithmetic,
and so don't understand what happens
operations exceed the supported range,
but the real confusion comes from mixed arithmetic, particularly
when mixing unsigned and signed values, as mentioned below:

\begin{plQuote}
{\citeAuthorYearTitle{JDK-4879804}}{}%
{%
A DESCRIPTION OF THE REQUEST: (Ranjith Mandala)

I've seen that there have been several requests 
for unsigned integral types in Java, 
and you have generally blown these off 
in a fairly cavalier manner. 
Usually you report that these types are unnecessary 
and can be worked around. In some cases you are correct, 
and I can write code that chews up CPU time 
masking and shifting bits around, 
using the char type as an unsigned temporary bucket, etc. 
However, I'm dealing with a stream of bytes 
that contains numeric values. 
I have to grab a few bytes at a time 
and make a number out of them---an integer, a long, whatever. 
Obviously, some of those bytes may have the high-order bit set, 
not because they are negative numbers 
but because that's how they fit in that particular byte 
in the sequence. 
Java's insistence on treating each byte as a signed value, 
and therefore doing sign-extension for me, is a major pain.

What this all boils down to is this: 
as a language vendor you should not be continually telling 
your user community that they are wrong 
when they request support from the language. 
Support the unsigned types because it is a reasonable thing to do, 
and stop pontificating. 
This is a major oversight in the language, and quite frankly, 
I'm amazed that you think it's okay to waste CPU run time 
rather than CPU compile time!

Joe Darcy added a comment - 2003-06-16 17:00

BT2:EVALUATION

If you want to more conveniently convert bytes to int or long, 
consider using nio byte buffers and viewing the byte buffer 
as an int or long buffer instead.

A parallel family of unsigned types was deliberately 
omitted from Java to avoid the confusion 
of combining signed and unsigned values in arithmetic expressions. 
That said, there are times
 when having unsigned arithmetic operations would be convenient. 
 However, if the bits of an int or long are interpreted 
 as unsigned two's complement numbers, 
 the output of add/subtract and multiply are 
 the same as if the numbers are interpreted as signed values 
 (this is a features of two's complement arithmetic). 
 Therefore, if you track signed-ness yourself, 
 you only need a separate unsigned divide method 
 and some conversion methods to have all the operations.}
\end{plQuote}

The real problem is trying to have one size fits all arithmetic.

When you have nested sets of numbers,you can get away with
automatic ``promotion'', 
although I think even there it is a mistake.
In other words, the set of numbers representable by
{\javaFont unsigned int} is a subset of those
representable by {\javaFont unsigned long},
so arithmetic mixing the two can be defined by identifying
with the {\javaFont unsigned int} values 
with the corresponding {\javaFont unsigned long}s,
and then using {\javaFont unsigned long} operations everywhere.

However, even in this simple case, mixed arithmetic is 
a source of lurking bugs.
There may be subexpressions involving only 
{\javaFont unsigned int} which would overflow/wraparound.
Might re-arrangements of the code may change whether the
wraparound happens or not, so a given intermediate value
may be $0$ in some cases, and $1+2^{32}$ in others.

I think the key problem is an invisible redefinition of 
``{\javaFont +}''.
Expressions in {\javaFont unsigned byte} implicitly define
{\javaFont +} as $\Add{\Naturals[0,0xFF]}$.
Change one argument to {\javaFont unsigned short} 
and {\javaFont +} is quietly redefined as
$\Add{\Naturals[0,0xFFFF]}$.
I suspect this leads to many developers forgetting about the
fact that the arithmetic is modular, and ignoring the possibility
of overflow.

The situation is worse, but not fundamentally different,
when you mixed number representations where neither set of
values is a subset of the other.
{\javaFont unsigned byte} is an implementation of
$\Naturals[0,255]$;
{\javaFont signed byte} implements $\Naturals[-128,127]$;
the union is $\Naturals[-128,255]$.
What should $+$ mean in that case?
I don't think anyone would find modular addition
on $[-128,255]$ an obvious choice (plus it would be expensive to 
implement).
The C standard, that Gosling was complaining about,
interprets the twos-complement bits of the signed number
as unsigned, effectively for signed/unsigned byte:
\begin{equation}
u + s \; \;=
\begin{cases}
\, u + s & \text{if } s \geq 0 \\
\, u + s + 256 & \text{if } s < 0 \\
\end{cases}
\end{equation}
which makes even less sense as ``generic addition''.

\TODO C-standard ref?

\NOTE Java's exclusion of unsigned integer types doesn't
eliminate the problem---the same issue arises in arithmetic mixing
{\javaFont long} and {\javaFont double},
where, despite it being described as a ``widening'' conversion 
\cite[section 5.1.2]{GoslingEtAl:2021:JLS16},
the set of numbers representable in neither of the two types
is a subset of the other.

So then. what could/should $+$ mean---when given operands
representing different subsets of $\Naturals$?
\begin{itemize}
  \item No overflow: Implement {\javaFont +} as 
  $\Add{\Naturals}$. In other words, whenever the value of 
  {\javaFont a + b} would be outside the subset of $\Naturals$
  covered by their implementations, return an implementation
  that can handle that value. 
  
  This could be done statically,
  {\javaFont 
  ((unsigned int) a) + ((unsigned int) b) -> unsigned long},
  for all {\javaFont unsigned int} {\javaFont a} and {\javaFont b},
  or dynamically, where the returned type depends whether 
  {\javaFont a + b} fits in an {\javaFont unsigned int} or not.
  
  Either way is very expensive. The static version would quickly
  promote all but the simplest expressions to an impractical
  unbounded representation.
  The dynamic version requires giving up primitive arithmetic
  for object-based. (at least, I don't know of any language 
  that supports dynamically changing which primitive type is
  returned from a function, though I think I can see how 
  that might be done in some kind of tagged architecture.)
  
  \TODO JMH code to measure the cost of static/dynamic promotion,
  at least for a proof-of-concept implementation.
  
  \item Overflow exceptions: 
  \href{https://github.com/palisades-lakes/sicpplus/blob/main/src/main/java/sicpplus/java/numbers/BoundedNatural.java}
{\javaFont BoundedNatural} and 
\href{https://github.com/openjdk/jdk/blob/master/src/java.base/share/classes/java/math/BigInteger.java}
{\javaFont BigInteger}
both behave this way.
  
  Java offers this in some cases (for signed integers)
  with {\javaFont Math.addExact}.
  
  This still has significant cost relative to instruction level
  modular addition.
  
  \TODO JHM code to measure this, at least p-o-c.
  
  \item All arithmetic is modular, 
  using a specific $\Add{\Naturals[n_0,n_1]}$.
  Mixing arguments from implementations covering different subsets
  of $\Naturals$ requires explicit conversion to a monoid
  $\Naturals[n_0,n_1]$ that includes all of them.
  The danger here is that it's easy to forget that the arithmetic
  is modular, because the $+$ we learned in elementary school
  wasn't, and it's also easy to lose track of exactly which
  $\Naturals[n_0,n_1]$ we are operating in 
  (though requiring explicit conversion helps.)
  
  One way to think about this is that it is exposing a fundamental
  difference between mathematics (as a language) and programming
  languages. 
  Mathematics, as practiced is full of ambiguity---one 
  of its great strengths I would argue.
  We can view $3$ as simultaneously an element of $\Naturals$
  and any of the modular subsets of $\Naturals$ that contain it.
  Or we can think of $\Naturals[n_0,n_1]$ as isomorphic to 
  a subset of $\Naturals$, with a very distinct addition operator.
  
  When we start talking about implementation, on the other hand,
  we have to make a choice. 
  
  Are {\javaFont unsigned byte},
  {\javaFont unsigned short}, {\ldots} are to be viewed as
  elements of distinct monoids (or semi-rings),
  with natural isomorphisms between matching subsets?
  Then requiring explicit conversion makes sense.
  
  Or do we want to think of them all as elements of one big set?
  Then, to get one big monoid, we need to pick one definition
  of {\javaFont +}, and the modular $\Add{\Naturals[n_0,n_1]}$
  for the enclosing $\Naturals[n_0,n_1]$ 
  is the only choice that works.
  This is equivalent to automatic promotion to the largest 
  word size, as discussed above.
  
\end{itemize}

\end{plSection}%{Implementation}
%-----------------------------------------------------------------
\end{plSection}%{Natural numbers}
%-----------------------------------------------------------------
\begin{plSection}{Integers}

\TODO Choose sans math font to match pseudocode font everywhere?

Suppose we have a problem to solve:
$a = b + c$
where $a,b,c\in \Naturals$ and we know $a$ and $b$,
but not $c$.

\begin{plAlgorithm}
{Reinventing subtraction}
{reinventSubtraction}
\begin{lstlisting}[language=pseudocode]
(defn $\Sub{\Naturals}$ [a b]
  (assert ($\Leq{\Naturals}$ 0 b a))
  (let [step (fn [c]
                    (if ($\Eq{\Naturals}$ a ($\Add{\Naturals}$ b c))
                      c
                      (step ($\Add{\Naturals}$ 1 c))]
    (step 0)))
\end{lstlisting}
\end{plAlgorithm}

%-----------------------------------------------------------------
\begin{plSection}{Implementation}
%-----------------------------------------------------------------
\begin{plSection}{\javaFont BigInteger}

\end{plSection}%{\javaFont BigInteger}
%-----------------------------------------------------------------
\begin{plSection}{Primitives}

\end{plSection}%{Primtives}
%-----------------------------------------------------------------
\end{plSection}%{Implementation}
%-----------------------------------------------------------------
\end{plSection}%{Integers}
%-----------------------------------------------------------------
\begin{plSection}{Rationals}


\end{plSection}%{Rationals}
%-----------------------------------------------------------------
\begin{plSection}{Classical Reals}


\end{plSection}%{Classical Reals}
%-----------------------------------------------------------------
\begin{plSection}{Computable Reals}

\end{plSection}%{Computable Reals}
%-----------------------------------------------------------------
\begin{plSection}{Hyperreals and infinitesimals}

\end{plSection}%{Hyperreals and infinitesimals}
%-----------------------------------------------------------------

\end{plSection}%{Number systems}

%-----------------------------------------------------------------
\input{typesetting}
%-----------------------------------------------------------------
%-----------------------------------------------------------------
\end{document}
%-----------------------------------------------------------------
